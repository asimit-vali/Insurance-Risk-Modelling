---
title:  "Insurance Risk Modelling: Fitting Loss Distributions"
author: "Prof Vali Asimit and Dr Feng Zhou"
output:
  bookdown::html_document2:
    self_contained: false
    toc: yes
    toc_depth: '3'
    number_sections: yes
    fig_width: 6
    fig_height: 4
    code_folding: hide
    css: WritingSupportFiles/style.css
    includes:
      in_header: WritingSupportFiles/ShowHide.js
  # bookdown::pdf_book:
  #   number_sections: yes
  #   toc: yes
  #   toc_depth: '3'
  #   template: [WritingSupportFiles\EWF-latex-ms.tex]
  #   keep_tex: true
  #   fig_caption: true
  #   latex_engine: pdflatex
bibliography: ["WritingSupportFiles/LDAReferenceC.bib","WritingSupportFiles/articles.bib","WritingSupportFiles/books.bib","WritingSupportFiles/packagesA.bib","WritingSupportFiles/packagesB.bib"]
biblio-style: WritingSupportFiles\econPeriod
keywords: "Bootstrapping, Data Visualisation, MLE, MOM"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontsize: 12pt
spacing: onehalf
---

<!-- ```{r echo = FALSE, child="WritingSupportFiles\\Packages.Rmd", warning=FALSE,message=FALSE} -->
<!-- ``` -->

```{r echo = FALSE, child='WritingSupportFiles/PdfOutput.Rmd', warning=FALSE,message=FALSE}
```

```{r echo = FALSE, child='WritingSupportFiles/Glossary.Rmd', warning=FALSE,message=FALSE}
```


# Introduction -- R Workspace and Data Exploration {#Intro}

***
In this section, you learn to:

- Set-up the your R workspace -- see Section \@ref(RInstall);
- How to describe the data to a general audience -- see Section \@ref(DD);
- Explore the trends in the data via descriptive tools -- see Section \@ref(EDA).

***

## Install R packages {#RInstall}

Install and activate the R packages for specific computations of this lab such as *fitdistrplus* and *actuar*.  Some additional generic R packages are needed such as *knitr*, *data.table*, etc.  Recall that this R lab is written in Bookdown setting and installing R packages should be done directly from RStudio.  If you run the R code in a normal fashion, use e.g. *install.packages("knitr")* to install the R package *knitr*. Activating R packages is possible by individual activation, e.g. *library(knitr)*, or by activating multiple R packages in one go, e.g. 

$$
libraries(c("fitdistrplus", "actuar", "knitr", "data.table")),
$$
but the latter requires a previous installation of the R package *easypackages*.
 

```{r, echo=T, comment = NA} 
#install.packages("knitr") ## example of individual R package installation
#library(knitr)            ## example of individual R package activation
#suppressWarnings(library(easypackages))
#library(easypackages)
#libraries(c("fitdistrplus", "actuar", "knitr", "data.table", "MASS", "survival")) ## example of multiple R package activation in one go
suppressWarnings(library(actuar, warn.conflicts = FALSE))  ## warning messages from activating "actuar" package are ignored
suppressWarnings(library(MASS))
suppressWarnings(library(survival))
suppressWarnings(library(fitdistrplus))
suppressWarnings(library(grDevices))
suppressWarnings(library(knitr))
suppressWarnings(library(data.table))
```

## Data Description {#DD}

The purpose of this section is to describe the data to a general audience. Our insurance data are given as `Building insurance claim size' and have a sample of size $1,502$ of 
$$(x_{i1}, x_{i2}, x_{i3},y_{i1},y_{i2}) \quad \text{with}\; 1\le i \le 1,502,$$
where $(x_{i1}, x_{i2}, x_{i3})$ are the *covariates* sampled from $(X_1,X_2,X_3)$ that help to predict the target variables $(Y_1,Y_2)$ via the observed sample $(y_{i1}, y_{i2})$.  Recall that $(Y_1,Y_2)$ represents the building and content claim that are recorded as *Building Insurance Claim Size* and *Content Insurance Claim Size* variables. Moreover, $(X_1,X_2,X_3)$ provides redundant information, since *X_1* is the *Recorded time* variable that contains the information given by *X_2* and *X_3* which records the claim occurrence time (*year* and *month*, respectively).  That is,

The feature values $\textbf{x}$'s include information about the following 

i) *Recorded time ($X_1$)* -- date variable;
ii) *Building Insurance Claim Size ($Y_1$)* -- continuous variable that records the *building* insurance claim; 
iii) *Content Insurance Claim Size ($Y_2$)* -- continuous variable that records the *content* insurance claim;
iv) *Year ($X_2$)* -- categorical variable with 11 possible values: from `1980` to `1990`;
v) *Month ($X_3$)* -- categorical variable with 12 possible values: from `1` representing January to `12`representing December.

The data file is named `Bivariate_Fire_Insurance_Clean_Data.csv` and a small `snapshoot` of the data is given in Table \@ref(tab:FireInsTable). A full *Exploratory Data Analaysis* is provided in Section \@ref(EDA), which explains the sample in greater details.

Our insurance data are given as `Building insurance claim size' and has the following structure:
```{r FireInsTable, echo=T, comment = NA} 
Bivariate_Fire_Insurance_Clean_Data <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
kable(head(Bivariate_Fire_Insurance_Clean_Data[,1:5]), caption="Fire Insurance Data Snapshoot",align = "rrrrr", format.args = list(big.mark = ","))
Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
#kable(head(Bivariate_Fire_Insurance_Clean_Data_toy), caption="Fire Insurance Data Snapshoot",align = "rrrrr", format.args = list(big.mark = ","))
summary(Bivariate_Fire_Insurance_Clean_Data_toy[,2:5])
```
Note that this R lab aims to provide a prediction model only the *Building* claims ($Y_1$), since the *Content* claims ($Y_2$) could be modelled in the same fashion. Keep in mind that $Y_1$ and $Y_2$ are dependent random variables. 

```{r CorrPlotY1Y2Figs, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=4, fig.cap = 'Correlation for $(Y_1,Y_2)$ for various years (left) and months (right)'}
suppressWarnings(library(corrplot))
par(mfrow=c(1,2))
### yearly correlation plot 
coryear <- c()
for(i in 1980:1990){coryear <- c(coryear, cor(Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),c("Y1","Y2")])[1,2])}
names(coryear) <-  1980:1990
plot(1980:1990,coryear, main="Yearly Correlation Plot", ylab="Corr", xlab="year", type="p", col="red")
### monthly correlation plot 
cormonth <- c()
nmonth <-  c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
for(i in 1:12){cormonth <- c(cormonth, cor(Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),c("Y1","Y2")])[1,2])}
names(cormonth) <- nmonth
plot(1:12,cormonth, main="Monthly Correlation Plot",ylab="Corr", xlab="month", type="p", col="red",xaxt="n") 
axis(side=1,at=1:12,labels=nmonth)
```

It is quite difficult to explain the yearly correlations from Figure \@ref(fig:CorrPlotY1Y2Figs), but the monthly correlations are quite intuitive.  The *Building* and *Content* claims are highly correlated over the hot weather -- as it could be seen in July and August -- when concomitant large claims are more likely to occur during that period of time.


## Exploratory Data Analysis (EDA) {#EDA}

The purpose of this section is to explore the trends in the data through descriptive tools, which could help with selecting the most appropriate data analysis through statistical evidence which is done in Section \@ref(RDA).  

Recall that the strength of association between the *Building* and *Content* claims has been examined in Section \@ref(DD), namely Figure \@ref(fig:CorrPlotY1Y2Figs). Since our real data analysis from Section \@ref(RDA) will be focused on *Building* claims, less detailed *qualitative data explorations* are made for *Content* claims in Section \@ref(EDA). We mainly look at how homogeneous the claims are over the time, though statistical evidence would be required to confirm the findings from this section. 

First, we look at two timeplots, namely Figures \@ref(fig:FireInsTSplotY1) and \@ref(fig:FireInsTSplotY2), which show the evolution over time of the *Building* and *Content* claims, receptively. 

```{r FireInsTSplotY1, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=10, fig.height=6, fig.cap = 'Building Insurance Claim Size Timeplot'}
Bivariate_Fire_Insurance_Clean_Data_toy$X1 <- as.Date(Bivariate_Fire_Insurance_Clean_Data_toy$X1, "%d/%m/%Y")
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
pp1 <- Bivariate_Fire_Insurance_Clean_Data_toy %>%
    ggplot( aes(x=X1, y=Y1)) +
    geom_area(fill="#00ffff", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("Building Insurance Claim Size (Y1)") +
    xlab("Recorded time (X1)")+
    theme_ipsum()
pp1 <- ggplotly(pp1)
pp1
```

```{r FireInsTSplotY2, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=10, fig.height=6, fig.cap = 'Content Insurance Claim Size Timeplot'}
#Bivariate_Fire_Insurance_Clean_Data_toy$X1 <- as.Date(Bivariate_Fire_Insurance_Clean_Data_toy$X1, "%d/%m/%Y")
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
pp1 <- Bivariate_Fire_Insurance_Clean_Data_toy %>%
    ggplot( aes(x=X1, y=Y2)) +
    geom_area(fill="#84ff00", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("Content Insurance Claim Size (Y2)") +
    xlab("Recorded time (X1)")+
    theme_ipsum()
pp1 <- ggplotly(pp1)
pp1
```

Figures \@ref(fig:FireInsTSplotY1) and \@ref(fig:FireInsTSplotY2) give an overall understanding of the claims, and we observe some outliers, i.e. very large claims as compared to the *common* claims. We should be mindful of such claims when fitting the claim amount distribution as such atypical claims may affect the robustness of our estimation.

The next step is to look at some classical histograms to understand whether the claims' pattern show unusual patterns in some years and/or months. Figures \@ref(fig:HistDataYearY1) and \@ref(fig:HistDataYearY2) show the yearly histograms for the *Building* and *Content* claims, respectively.


```{r HistDataYearY1, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=9, fig.cap = 'Building Insurance Claim Size (Y1) -- Yearly Histograms'} 
par(mfrow=c(3,4))
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 hist(xxx$Y1,main="", xlab=i, border="red", col="green")}
hist(Bivariate_Fire_Insurance_Clean_Data_toy$Y1, main="", xlab="All Years", border="red", col="green")
```

```{r HistDataYearY2, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=9, fig.cap = 'Content Insurance Claim Size (Y2) -- Yearly Histograms'} 
par(mfrow=c(3,4))
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 hist(xxx$Y2,main="", xlab=i, border="red", col="green")}
hist(Bivariate_Fire_Insurance_Clean_Data_toy$Y2, main="", xlab="All Years", border="red", col="green")
```

Figures \@ref(fig:HistDataMonthY1) and \@ref(fig:HistDataMonthY2) show the monthly histograms for the *Building* and *Content* claims, respectively.

```{r HistDataMonthY1, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=9, fig.cap = 'Building Insurance Claim Size (Y1) -- Monthly Histograms'} 
par(mfrow=c(3,4))
nmonth <-  c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 hist(xxx$Y1,main="", xlab=nmonth[i], border="red", col="green")}
```

```{r HistDataMonthY2, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=9, fig.cap = 'Content Insurance Claim Size (Y2) -- Monthly Histograms'} 
par(mfrow=c(3,4))
nmonth <-  c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 hist(xxx$Y2,main="", xlab=nmonth[i], border="red", col="green")}
```

It is quite clear that interpreting numerous histograms -- displayed as Figures \@ref(fig:HistDataYearY1) to \@ref(fig:HistDataMonthY2) -- is not convenient, and thus, we now report summary statistics in some radar plots that could help gathering and comparing the yearly and monthly risks with ease.  

The following radar plots are provided for the **Building Insurance Claim Size (Y1)**:

i) Figure \@ref(fig:RadarPlotYearY1e) that displays the *yearly* mean and standard deviation;
ii) Figure \@ref(fig:RadarPlotYearY1q)  that displays the *yearly* quantiles;
iii) Figure \@ref(fig:RadarPlotMonthY1e) that displays the *monthly* mean and standard deviation;
iv) Figure \@ref(fig:RadarPlotMonthY1q) that displays the *monthly* quantiles.

Note that the claims are divided by *1,000,000*, i.e. all measures of risks -- mean, standard deviation and quantiles -- are scaled down by a factor of *1,000,000*.


```{r RadarPlotYearY1e, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Building Insurance Claim Size (Y1) -- Yearly Summary Statistics'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
meanyear <- numeric(11)
sdyear <- numeric(11)
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 meanyear[i-1979] <- mean(xxx$Y1)/1000000
 sdyear[i-1979] <- sd(xxx$Y1)/1000000}
library(plotly)
fig1eY1 <- plot_ly(
    type = 'scatterpolar',
    r = c(meanyear, meanyear[1]),
    theta=paste0("year ",as.character(c(1980:1990,1980))), #the labels are forced to be characters, so the setting from below does not work
               # theta = c("1980", "1981", "1982", "1983", "1984", "1985", "1986'", "1987", "1988", "1989", "1990", "1980"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'Average Claim/1,000,000'
  ) 
fig1eY1 <- fig1eY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(sdyear, sdyear[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dotdash"),
  name = 'Standard Deviation Claim/1,000,000'
  ) 
fig1eY1
```

```{r RadarPlotYearY1q, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Building Insurance Claim Size (Y1) -- Yearly Summary Statistics (quantiles)'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
q50year <- numeric(11)
q75year <- numeric(11)
q95year <- numeric(11)
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 q50year[i-1979] <- quantile(xxx$Y1,0.50)/1000000
 q75year[i-1979] <- quantile(xxx$Y1,0.75)/1000000
 q95year[i-1979] <- quantile(xxx$Y1,0.95)/1000000}
library(plotly)
fig1qY1 <- plot_ly(
    type = 'scatterpolar',
    r = c(q50year, q50year[1]),
    theta=paste0("year ",as.character(c(1980:1990,1980))), 
    #fill = 'toself',
    linetype = I("dot"),
    name = 'q(50%)/1,000,000'
  ) 
fig1qY1 <- fig1qY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(q75year, q75year[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dashdot"),
  name = 'q(75%)/1,000,000'
  ) 
fig1qY1 <- fig1qY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(q95year, q95year[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dash"),
  name = 'q(95%)/1,000,000'
  ) 
fig1qY1
```

```{r RadarPlotMonthY1e, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Building Insurance Claim Size (Y1) -- Monthly Summary Statistics'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
meanmonth <- numeric(11)
sdmonth <- numeric(11)
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 meanmonth[i] <- mean(xxx$Y1)/1000000
 sdmonth[i] <- sd(xxx$Y1)/1000000}
library(plotly)
fig1eY1 <- plot_ly(
    type = 'scatterpolar',
    r = c(meanmonth, meanmonth[1]),
    theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'Average Claim/1,000,000'
  ) 
fig1eY1 <- fig1eY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(sdmonth, sdmonth[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dotdash"),
  name = 'Standard Deviation Claim/1,000,000'
  ) 
fig1eY1
```

```{r RadarPlotMonthY1q, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Building Insurance Claim Size (Y1) -- Monthly Summary Statistics (quantiles)'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
q50month <- numeric(11)
q75month <- numeric(11)
q95month <- numeric(11)
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 q50month[i] <- quantile(xxx$Y1,0.50)/1000000
 q75month[i] <- quantile(xxx$Y1,0.75)/1000000
 q95month[i] <- quantile(xxx$Y1,0.95)/1000000}
library(plotly)
fig1qY1 <- plot_ly(
    type = 'scatterpolar',
    r = c(q50month, q50month[1]),
    theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'q(50%)/1,000,000'
  ) 
fig1qY1 <- fig1qY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(q75month, q75month[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dashdot"),
  name = 'q(75%)/1,000,000'
  ) 
fig1qY1 <- fig1qY1 %>% add_trace(
  type = 'scatterpolar',
  r = c(q95month, q95month[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dash"),
  name = 'q(95%)/1,000,000'
  ) 
fig1qY1
```

*Building* claims show an interesting trend. While Figure \@ref(fig:RadarPlotYearY1e) shows large variation -- coefficient of variation larger than 1 -- on the yearly basis, the monthly claims from Figure \@ref(fig:RadarPlotMonthY1e) show the same behaviour but only for July, August and October. This peculiar behaviour could be explained by the lack of homogeneity on the annual basis, i.e. the iid (independent and identical distributed) assumption may not hold for the individual claims (sampled annually).  This is only our initial intuition, though statistical testing could confirm or not this behaviour. At the same time, the timplot from Figures \@ref(fig:FireInsTSplotY1) shows three extreme claims; the most extreme claim appears in 1980, and the second(third) most extreme claim is recorded in 1985(1988), which is in accordance with Figure \@ref(fig:RadarPlotYearY1e). Quantiles could better explain the tail behaviour, and thus, the $50\%$, $75\%$ and $95\%$ quantiles are plotted in Figures \@ref(fig:RadarPlotYearY1q) and \@ref(fig:RadarPlotMonthY1q).  The same outliers -- explained before -- tend to explain the large difference between the $75\%$ and $95\%$ quantiles, especially in year 1985 and month October. 


*Content* claims show an even more extreme behaviour -- see Figure \@ref(fig:RadarPlotYearY2e) to \@ref(fig:RadarPlotMonthY2q) -- which is in accordance with multiple unusually large claims observed in the timeplot from Figures \@ref(fig:FireInsTSplotY2). The last visualisation tools of this section are now given for **Content Insurance Claim Size (Y2)**, i.e. teh following radar plots:

i) Figure \@ref(fig:RadarPlotYearY2e) that displays the *yearly* mean and standard deviation;
ii) Figure \@ref(fig:RadarPlotYearY2q)  that displays the *yearly* quantiles;
iii) Figure \@ref(fig:RadarPlotMonthY2e) that displays the *monthly* mean and standard deviation;
iv) Figure \@ref(fig:RadarPlotMonthY2q) that displays the *monthly* quantiles.

Note that the claims are divided by *1,000,000*, i.e. all measures of risks -- mean, standard deviation and quantiles -- are scaled down by a factor of *1,000,000*.


```{r RadarPlotYearY2e, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Content Insurance Claim Size (Y2) -- Yearly Summary Statistics'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
meanyear <- numeric(11)
sdyear <- numeric(11)
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 meanyear[i-1979] <- mean(xxx$Y2)/1000000
 sdyear[i-1979] <- sd(xxx$Y2)/1000000}
library(plotly)
fig1eY2 <- plot_ly(
    type = 'scatterpolar',
    r = c(meanyear, meanyear[1]),
    theta=paste0("year ",as.character(c(1980:1990,1980))), #the labels are forced to be characters, so the setting from below does not work
               # theta = c("1980", "1981", "1982", "1983", "1984", "1985", "1986'", "1987", "1988", "1989", "1990", "1980"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'Average Claim/1,000,000'
  ) 
fig1eY2 <- fig1eY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(sdyear, sdyear[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dotdash"),
  name = 'Standard Deviation Claim/1,000,000'
  ) 
fig1eY2
```

```{r RadarPlotYearY2q, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Content Insurance Claim Size (Y2) -- Yearly Summary Statistics (quantiles)'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
q50year <- numeric(11)
q75year <- numeric(11)
q95year <- numeric(11)
for(i in 1980:1990){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X2==i),]
 q50year[i-1979] <- quantile(xxx$Y2,0.50)/1000000
 q75year[i-1979] <- quantile(xxx$Y2,0.75)/1000000
 q95year[i-1979] <- quantile(xxx$Y2,0.95)/1000000}
library(plotly)
fig1qY2 <- plot_ly(
    type = 'scatterpolar',
    r = c(q50year, q50year[1]),
    theta=paste0("year ",as.character(c(1980:1990,1980))), 
    #fill = 'toself',
    linetype = I("dot"),
    name = 'q(50%)/1,000,000'
  ) 
fig1qY2 <- fig1qY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(q75year, q75year[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dashdot"),
  name = 'q(75%)/1,000,000'
  ) 
fig1qY2 <- fig1qY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(q95year, q95year[1]),
  theta=paste0("year ",as.character(c(1980:1990,1980))),
  #fill = 'toself',
  linetype = I("dash"),
  name = 'q(95%)/1,000,000'
  ) 
fig1qY2
```

```{r RadarPlotMonthY2e, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Content Insurance Claim Size (Y2) -- Monthly Summary Statistics'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
meanmonth <- numeric(11)
sdmonth <- numeric(11)
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 meanmonth[i] <- mean(xxx$Y2)/1000000
 sdmonth[i] <- sd(xxx$Y2)/1000000}
library(plotly)
fig1eY2 <- plot_ly(
    type = 'scatterpolar',
    r = c(meanmonth, meanmonth[1]),
    theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'Average Claim/1,000,000'
  ) 
fig1eY2 <- fig1eY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(sdmonth, sdmonth[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dotdash"),
  name = 'Standard Deviation Claim/1,000,000'
  ) 
fig1eY2
```

```{r RadarPlotMonthY2q, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=9, fig.height=5, fig.cap = 'Content Insurance Claim Size (Y2) -- Monthly Summary Statistics (quantiles)'} 
#Bivariate_Fire_Insurance_Clean_Data_toy <- read.table("Bivariate_Fire_Insurance_Clean_Data.csv",header=TRUE,as.is=TRUE,sep=",")
#colnames(Bivariate_Fire_Insurance_Clean_Data_toy) <- c("X1", "Y1", "Y2","X2", "X3")
q50month <- numeric(11)
q75month <- numeric(11)
q95month <- numeric(11)
for(i in 1:12){
 xxx <- Bivariate_Fire_Insurance_Clean_Data_toy[which(Bivariate_Fire_Insurance_Clean_Data_toy$X3==i),]
 q50month[i] <- quantile(xxx$Y2,0.50)/1000000
 q75month[i] <- quantile(xxx$Y2,0.75)/1000000
 q95month[i] <- quantile(xxx$Y2,0.95)/1000000}
library(plotly)
fig1qY2 <- plot_ly(
    type = 'scatterpolar',
    r = c(q50month, q50month[1]),
    theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
    #fill = 'toself',
    linetype = I("dot"),
    name = 'q(50%)/1,000,000'
  ) 
fig1qY2 <- fig1qY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(q75month, q75month[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dashdot"),
  name = 'q(75%)/1,000,000'
  ) 
fig1qY2 <- fig1qY2 %>% add_trace(
  type = 'scatterpolar',
  r = c(q95month, q95month[1]),
  theta = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan"),
  #fill = 'toself',
  linetype = I("dash"),
  name = 'q(95%)/1,000,000'
  ) 
fig1qY2
```

This ends our EDA that is a preamble of the real data analysis provided in Section \@ref{RDA}. We next provide a detailed simulation study in Section \ref{SSA} that relies on the same loss data estimation procedures from Section \@ref{RDA}.  The main aim of the simulation study is to provide the possible shortcomings of statistical models used to fit our insurance data.

# Simulation Study {#SSA}

***

In this section, you learn to:

- Simulate synthetic data from univariate (loss) distributions  -- see Section \@ref(SD);
- Fit data via **Maximum Likelihood Estimation (MLE)**  -- see Section \@ref(MLE);
- Fit data via **Method of Moments (MOM)** -- see Section \@ref(MOM);
- Understand and explain your **MLE** and **MOM** estimates -- see Sections \@ref(S1), \@ref(S2) and \@ref(S3).

***

## Simulate Data {#SD}

This section provides elementary simulations from univariate (loss) distributions.  We first simulate samples of size $n=500$ -- though samples of size $n=\{25, 50, 100, 250, 500\}$ will be later generated -- from the following three parametric distributions:

i) *LogNormal* with parameters $\mu=10$ and $\sigma=1$ known as **Sample 1** -- note that all *LogNormal* distributions are **moderately heavy** tailed distributions;
ii) *Weibull* with parameters $a=0.5$ and $b=1$, known as **Sample 2** -- note that this particular *Weibull* is a **moderately heavy** tailed distribution since $a<1$; 
iii) *Weibull* with parameters $a=2$ and $b=1$, known as **Sample 2** -- note that this particular *Weibull* is a **very light** tailed distribution since $a>1$; 

Note that our *LogNormal* distribution has the usual parametrisation, i.e. its expected value is $\exp\left(\mu+\frac{\sigma^2}{2}\right)$. Further, our *Weibull* parametrisation, which is in terms of $a,b>0$, has the following cumulative distribution function: 

$$
F_{Weibull}(x;a,b)=\Pr(X\le x)=\exp\left\{-\left(\frac{x}{b}\right)^a\right\},\quad\text{for all}\;x\ge0,\;\text{i.e.}\;\mathbb{E}[X]=b\cdot\Gamma\left(1+\frac{1}{a}\right), 
$$
where $\Gamma()$ is the usual Gamma function. 

The summary statistics for the three samples are given below: 


```{r}
set.seed(123); #set your seed so you could replicate the same sample every time you run the R script
n=500;  
#Simulate Sample 1: logNormal($\mu=10, \sigma=1$) moderate heavy tail
Sample1 <- rlnorm(n, meanlog=10, sdlog=1)  
summary(Sample1)
#Simulate Sample 2: Weibull($a=0.5, b=1$) moderate heavy tail
Sample2 <- rweibull(n, shape=0.5 , scale=1) 
#Note that the scale parameter b=1 
summary(Sample2)
#Simulate Sample 3: Weibull($a=2, b=1$) very light tail
Sample3 <- rweibull(n, shape=2 , scale=1)
summary(Sample3)
```

Let us plot the histograms for three samples, which are given in Figure \@ref(fig:HistSimData) below.

```{r HistSimData, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=9, fig.cap = 'Histograms for Sample 1 and log of Sample 1 (top), and Samples 2 and 3 (bottom)'} 
par(mfrow=c(2,2))
hist(Sample1, prob=TRUE, border="red", col="green");
hist(log(Sample1), prob=TRUE, border="red", col="green");
hist(Sample2, prob=TRUE, border="red", col="green");
hist(Sample3, prob=TRUE, border="red", col="green");
```
As expected, Samples 1 and 2 histograms are right skewed in Figure \@ref(fig:HistSimData) due to their heavy tailness, while Sample 3 shows a very different pattern. At a first glance, Sample 2 seems to come from a less heavy tailed distribution than Sample 1 since one may say the Sample 1 histogram fades away after the threshold $3\times10^5$, while Sample 2 histogram fades away after the threshold $30$; this argument is faulty since the two distributions calibrate distributions with very different moments, e.g. the theoretical expected value of Sample 1 and Sample 2 is $\exp(10.5)=36,315.50$ and $1\cdot\Gamma\left(1+\frac{1}{0.5}\right)=2$, respectively.  In fact, one could show that the theoretical distribution of *Weibull($a=0.5$,$b=1$)* exhibits a heavier tail distribution than 
*LogNormal($\mu=10$,$\sigma=1$)*.  

Finally, the log-transformation of the Sample 1 data leads to a symmetric histogram due to the fact that $\log(X)\sim N\big(\mu,\sigma^2\big)$ whenever $X\sim LogN\big(\mu,\sigma^2\big)$.

## Maximum Likelihood Estimation (MLE) {#MLE}

***

We are now ready to parametrically fit our sample data from Section \@ref(SD), and two methods are chosen:

- **Maximum Likelihood Estimation (MLE)**  -- see Section \@ref(MLE) -- where the parameters are chosen so that the likelihood of observing the given sample is maximised;
- **Method of Moments (MOM)** -- see Section \@ref(MOM) -- where sample and theoretical higher moments are matched, i.e are equal.

MLE and MOM estimations are possible via function **mledist** and **mmedist** functions that are available in the *fitdistrplus* R package. **There are numerous numerical issues with both MLE and MOM implementations in R especially when fitting distributions with more than one parameter.**  Why?  

*MLE* requires solving non-linear optimisation problems, and R (and Python) rely on basic optimisation tools that are designed for general use, and bespoke software like MATLAB would be more computationally stable; this issue is more acute when the dimension of the optimisation problem, i.e. number of parameters gets larger (two or more parameters). 

*MOM* usually requires solving a non-linear system of equations, which is computationally feasible even in R (and Python) -- as an application of the *Fixed Point Theorem* though other methods are available -- when one parameter distribution is fitted, but *MOM* is computationally more challenging (not only in R and Python) when the number of parameters gets larger (two or more parameters). 

***

Eight parametric families are used to fit our three samples. The standard R notations are used to signify which parametric we refer to. in the two packages for 

1) **lnorm** refers to *LogNormal distribution* -- see *actuar* and *stats* R packages;
2) **exp** refers to *Exponential distribution* -- see *actuar* and *stats* R packages;
3) **gamma** refers to *Gamma distribution* -- see *actuar* and *stats* R packages;
4) **weibull** refers to *Weibull distribution* -- see *actuar* and *stats* R packages;
5) **llogis** refers to *LogLogistic distribution* -- see *actuar* R package;
6) **invweibull** refers to *Inverse Weibull distribution* -- see *actuar* R package;
7) **pareto** refers to *Pareto distribution* -- see *actuar* R package;
8) **trgamma** refers to *transformed Gamma distribution* -- see *actuar* R package.

Recall that **pareto** is defined on $(0, \infty)$, which is not true for **Pareto Type 1**, and thus, we prefer using **pareto** so that all eight parametric families are defined on the same range. 

*MLE* estimation requires a `good' starting point for their numerical algorithms, and thus, one should be careful when performing MLE estimation for two or more parametric distributions;  local minima or even saddle points could be obtained, which are far from the desired global optimum point/points if this/these exists/exist as the likelihood function is not always a convex function. *MOM* faces numerical shortcomings when performing MLE estimation for two or more parametric distributions. Numerical issues are signified as **NA** in the R output.

Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **lnorm** family) as follows:
```{r}
MLE.lnorm=mledist(Sample1, "lnorm")
MLE.lnorm$estimate
```

Sample 1 (simulated from *LogNormal*) is fitted via MLE assuming **exp** family as follows:
```{r}
MLE.exp=mledist(Sample1/1000, "exp") 
#Large valued data (not large sized samples) require to scale down the data by a factor 
#we chose here to scale down by a factor of 1,000 since the expected value is around \exp(10.5)=36,315.50
#we should scale back the MLE estimate, and since we actually fit the rate, we divide the MLE estimate by 1,000
#recall that expected value is equal to 1/rate
MLE.exp$estimate/1000
```

Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **gamma** family) as follows:
```{r}
MLE.gamma=mledist(Sample1, "gamma",lower=c(0, 0), start=list(shape =1, rate=1))
#A `good' starting point is needed here
MLE.gamma$estimate  #rate=1/scale
```

Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **weibull** family) as follows:
```{r}
MLE.weibull=mledist(Sample1, "weibull")
MLE.weibull$estimate
```

Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **llogis** family) as follows:
```{r}
MLE.llogis=mledist(Sample1, "llogis")
MLE.llogis$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **invweibull** family) as follows:
```{r}
MLE.invweibull=mledist(Sample1, "invweibull")
MLE.invweibull$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **pareto** family) as follows:
```{r}
MLE.pareto=mledist(Sample1, "pareto",lower=c(0, 0), start=list(shape =1, scale=1))
#A `good' starting point is needed here
MLE.pareto$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **trgamma** family) as follows:
```{r}
MLE.trgamma=mledist(Sample1, "trgamma")
MLE.trgamma$estimate
```

MLE *fitted* distributions have their density plots in Figure \@ref(fig:MLEpdfPlots), though there is not much to say about these plots.   

```{r MLEpdfPlots, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=6, fig.cap = 'MLE Density Plots'} 
par(mfrow=c(2,4))
curve(dlnorm(x,MLE.lnorm$estimate[1],MLE.lnorm$estimate[2]),from=0,to=max(Sample1),ylab="Density",  col=1, main="lnorm", xlab = "")
curve(dexp(x,MLE.exp$estimate/1000),from=0,to=max(Sample1),ylab="Density",  col=2, main="exp", xlab = "")
curve(dgamma(x,MLE.gamma$estimate[1],MLE.gamma$estimate[2]),from=0,to=max(Sample1),ylab="Density",  col=3, main="gamma", xlab = "")
curve(dweibull(x,MLE.weibull$estimate[1],MLE.weibull$estimate[2]),from=0, to = max(Sample1), ylab="Density",col=4,main="weibull", xlab = "")
curve(dllogis(x,MLE.llogis$estimate[1],MLE.llogis$estimate[2]),from=0, to = max(Sample1), ylab="Density",col=5,main="llogis", xlab = "")
curve(dinvweibull(x,MLE.invweibull$estimate[1],MLE.invweibull$estimate[2]),from=0, to = max(Sample1), ylab="Density",col=6,main="invweibull", xlab = "")
curve(dpareto(x,MLE.pareto$estimate[1],MLE.pareto$estimate[2]),from=0, to = max(Sample1), ylab="Density",col=7,main="pareto", xlab = "")
curve(dtrgamma(x,MLE.trgamma$estimate[1],MLE.trgamma$estimate[2],MLE.trgamma$estimate[3]),xlim=c(0,max(Sample1)),ylab="Density",col=8, main="trgamma", xlab = "")
```



## Method of Moments {#MOM}

*MOM* usually requires solving a non-linear system of equations, which is often computationally infeasible when MOM is applied for parametric families with two or more parameters -- also mentioned at the beginning of Section \@ref(MLE). Sample 1 data are fitted via MOM through the **mmedist** function, and you could see that MOM estimation fails in some cases as we anticipated.  

Sample 1 (simulated from *LogNormal*) is fitted via MOM (assuming **lnorm** family) as follows:
```{r}
MME.lnorm=mmedist(Sample1, "lnorm")
MME.lnorm$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **exp** family) as follows:
```{r}
MME.exp=mmedist(Sample1, "exp")
MME.exp$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **gamma** family) as follows:
```{r}
MME.gamma=mmedist(Sample1, "gamma")
MME.gamma$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **weibull** family) as follows:
```{r}
memp  <-  function(x, order) mean(x^order)
MME.weibull=mmedist(Sample1, "weibull", order=c(1, 2), memp=memp)
MME.weibull$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **llogis** family) as follows:
```{r}
memp  <-  function(x, order) mean(x^order)
MME.llogis=mmedist(Sample1, "llogis", order=c(1, 2), memp=memp)
MME.llogis$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **invweibull** family) as follows:
```{r}
memp  <-  function(x, order) mean(x^order)
MME.invweibull=mmedist(Sample1, "invweibull", order=c(1, 2), memp=memp)
MME.invweibull$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **pareto** family) as follows:
```{r}
memp  <-  function(x, order) mean(x^order)
MME.pareto=mmedist(Sample1, "pareto", order=c(1, 2), memp=memp)
MME.pareto$estimate
```
Sample 1 (simulated from *LogNormal*) is fitted via MLE (assuming **trgamma** family) as follows:
```{r}
memp  <-  function(x, order) mean(x^order)
MME.trgamma=mmedist(Sample1, "trgamma", order=c(1, 2, 3), memp=memp)
MME.trgamma$estimate
```

There are no obvious concerns with MOM estimates when fitting **lnorm**, **exp** and **gamma** families. There is no doubt that MOM fails when fitting **llogis** and **trgamma** families, and there is no immediate remedy. MOM estimation when fitting **weibull**, **invweibull** and **pareto** families provide some caution messages, and thus, we check whether the empirical and theoretical two moments are matched as it supposed to be.

```{r}
xxx <- c(mean(Sample1),mean(Sample1^2))
xxxW<-c(mweibull(1, shape=MME.weibull$estimate[1], scale = MME.weibull$estimate[2]),mweibull(2, shape=MME.weibull$estimate[1], scale = MME.weibull$estimate[2]))
xxxIW<-c(minvweibull(1, shape=MME.invweibull$estimate[1], scale = MME.invweibull$estimate[2]),minvweibull(2, shape=MME.invweibull$estimate[1], scale = MME.invweibull$estimate[2]))
xxxP<-c(mpareto(1, shape=MME.pareto$estimate[1], scale = MME.pareto$estimate[2]),mpareto(2, shape=MME.pareto$estimate[1], scale = MME.pareto$estimate[2]))
cat("First two (raw) moments of Samples 1 -- empirical estimates")
xxx
cat("First two (raw) moments of Samples 1 -- theoretical MOM weibull")
xxxW
cat("First two (raw) moments of Samples 1 -- theoretical MOM invweibull")
xxxIW
cat("First two (raw) moments of Samples 1 -- theoretical MOM pareto")
xxxP
cat("The diferences between the empirical and theoretical MOM for weibull, invweibull and pareto")
xxx-xxxW
xxx-xxxIW
xxx-xxxP
```

This suggests that the **weibull** and **invweibull** MOM estimates are very off, while the **pareto** MOM estimation is fine as it is.  

One could help R to better estimate the **weibull** MOM estimates from first principles. That is, we have to solve the following system of equations:
$$\mathbb{E}[X]=b\cdot\Gamma\left(1+\frac{1}{a}\right)=m_1\quad\text{and}\quad\mathbb{E}\big[X^2\big]=b^2\cdot\Gamma\left(1+\frac{2}{a}\right)=m_2,$$
where $m_1$ and $m_2$ are the first and second empirical moment estimates. That is, we need to find the MOM estimate of the shape parameter $\hat{a}_{MOM}>0$ such that 
$$\frac{m_2}{(m_1)^2}\left(\Gamma\left(1+\frac{1}{a}\right)\right)^2-\Gamma\left(1+\frac{2}{a}\right)=0,$$
which is a root problem that could be solved in R with the help of the *uniroot* function.  Clearly, the MOM estimate of the scale parameter is given by
$$\hat{b}_{MOM}=\frac{m_1}{\Gamma\left(1+\frac{1}{\hat{a}_{MOM}}\right)}.$$


Now, let us solve the root equation defined earlier, and we get that
```{r}
xxx <- c(mean(Sample1),mean(Sample1^2))
fW <- function(x) {(xxx[2]/xxx[1]^2)*(gamma(1+(1/x)))^2-gamma(1+(2/x))}
asol<-uniroot(fW, c(0.1,1))$root
bsol<-xxx[1]/gamma(1+(1/asol))
c(asol,bsol)
xxxW<-c(mweibull(1, shape=asol, scale = bsol),mweibull(2, shape=asol, scale = bsol))
cat("The diferences between the empirical and theoretical MOM for weibull")
xxx - xxxW
cat("Weibull MOM estimates -- (a=shape, b=scale)")
c(asol,bsol)
```

As you could see, the default toleration in the *uniroot* function does not lead to reasonable **weibull** MOM estimates. Therefore, we reduce the toleration to $tol=10^{-15}$ and narrow down the search interval to $(0.7,0.9)$ from the previous search interval of $(0.1,1)$, and we get that

```{r}
#fW <- function(x) {(xxx[2]/xxx[1]^2)*(gamma(1+(1/x)))^2-gamma(1+(2/x))}
asol1<-uniroot(fW, c(0.7,0.9), tol=10^(-15))$root
bsol1<-xxx[1]/gamma(1+(1/asol1))
xxxW1<-c(mweibull(1, shape=asol1, scale = bsol1),mweibull(2, shape=asol1, scale = bsol1))
cat("The diferences between the empirical and theoretical MOM for weibull")
xxx -xxxW1
cat("Weibull MOM estimates -- (a=shape, b=scale)")
c(asol1,bsol1)
```
The above estimates are reasonable since the empirical and theoretical moments are different up to an error of $10^{-7}$, even though there are small differences (default toleration and reduced toleration) between the MOM outputs, $\big(\hat{a}_{MOM},\hat{b}_{MOM}\big)$. 

It is a good exercise for you to try finding a remedy for the **invweibull** MOM estimation, which is similar to the **weibull** remedy from above.

As a final note, you should be aware that some ad-hoc MOM estimation are based on dispersion measures as well. For example, if one fits **gamma** family that has two parameters, one may try matching the first moment and variance, instead of matching the first and second moments. This is not a big issue, though variance could be estimated via the unbiased or the asymptotically unbiased estimator -- each having its pros and cons depending on the sample size -- and the two choices lead to different set of estimates; recall that even a small difference in parameter estimates could lead to massive model error as we noticed in the previous **weibull** MOM estimation.   


## LogNormal Sample -- Further Analysis {#S1}

***
In this section, you learn to:
  
- Compare competitive fitting models;
- Use bootstrapping to robustify your estimation;
- Understand the effect of sample size over the objectives from above.

***

So far, **Sample 1** is drawn from a *LogNormal* distribution -- a **moderately heavy** tailed distribution -- with parameters $\mu=10$ and $\sigma=1$. The sample of size of **Sample 1** has been $n=500$, and now take five samples of size $n=\{25, 50, 100, 250, 500\}$ as follows: 
```{r}
set.seed(1234) #Generate Sample1 and save in a list
n=c(25, 50, 100, 250, 500)
lnorm.meanlog=10
lnorm.sdlog=1
Sample1.list=list(n25=rep(0,25),n50=rep(0,50),n100=rep(0,100),n250=rep(0,250),n500=rep(0,500))
for (i in 1:length(n)) {
Sample1=rlnorm(n[i], meanlog=lnorm.meanlog, sdlog=lnorm.sdlog)
Sample1.list[[i]]=Sample1
}
```

We then perform a parametric estimation -- via *MLE* and *MOM* -- for all five samples when the underlying distribution is one of the eight parametric families considered before. 

Note that our *MLE* estimates from this section are as given by the R function *mledist* without checking whether these estimates are points of global maxima, which is beyond the scope of this session. Section \@ref(MLE) has detailed what one should do to enhance our confidence in our *MLE* estimates. 

Note also that *MOM* estimation is as given by the R function *mmedist*, which we have already seen how off could be for some parametric choices; specifically, when $n=500$, **weibull** and **invweibull** MOM estimations are not acceptable, though the **pareto** estimates turned out to be fine. We will not repeat the same exercise as in Section \@ref(MOM) to rectify these shortcomings, but one should keep in mind these possible drawbacks.

Here is the *MLE* and *MOM* estimation for all samples and parametric assumptions. 
```{r}
#MLE and MME for 8 different distributions and saved in each matrix
M1.MLE.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MLE.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MLE.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MLE.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MLE.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MLE.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MLE.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MLE.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
M1.MME.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MME.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MME.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MME.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MME.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MME.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MME.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MME.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
for (i in 1:length(n)) {
#lnorm
M1.MLE.lnorm[i,]=mledist(Sample1.list[[i]], "lnorm")$estimate
M1.MME.lnorm[i,]=mmedist(Sample1.list[[i]], "lnorm")$estimate
#exp
MLE.exp=mledist(Sample1.list[[i]]/1000, "exp")
#if (is.na(MLE.exp$estimate)==is.na(NA)) {MLE.exp$estimate=1/mean(Sample1.list[[i]])}
M2.MLE.exp[i,]=MLE.exp$estimate/1000
M2.MME.exp[i,]=mmedist(Sample1.list[[i]], "exp")$estimate
#gamma
MLE.gamma=mledist(Sample1.list[[i]], "gamma",lower=c(0, 0), start=list(shape =1, rate=1))  #when the meanlog is too large, become NA
M3.MLE.gamma[i,]=MLE.gamma$estimate
#if (is.na(MLE.gamma$estimate[1])==is.na(NA)) {
#  MLE.gamma$estimate=gammaMLE(Sample1.list[[i]])
#  MLE.gamma=c(MLE.gamma$estimate[1],1/MLE.gamma$estimate[2])
#  M3.MLE.gamma[i,]=MLE.gamma}
M3.MME.gamma[i,]=mmedist(Sample1.list[[i]], "gamma")$estimate
#weibull
M4.MLE.weibull[i,]=mledist(Sample1.list[[i]], "weibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.weibull=mmedist(Sample1.list[[i]], "weibull", order=c(1, 2), memp=memp)
M4.MME.weibull[i,]=MME.weibull$estimate
#llogis
M5.MLE.llogis[i,]=mledist(Sample1.list[[i]], "llogis")$estimate
memp  <-  function(x, order) mean(x^order)
MME.llogis=mmedist(Sample1.list[[i]], "llogis", order=c(1, 2), memp=memp)
M5.MME.llogis[i,]=MME.llogis$estimate
#invweibull  
M6.MLE.invweibull[i,]=mledist(Sample1.list[[i]], "invweibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.invweibull=mmedist(Sample1.list[[i]], "invweibull", order=c(1, 2), memp=memp)
M6.MME.invweibull[i,]=MME.invweibull$estimate
#pareto
M7.MLE.pareto[i,]=mledist(Sample1.list[[i]], "pareto")$estimate
memp  <-  function(x, order) mean(x^order)
MME.pareto=mmedist(Sample1.list[[i]], "pareto", order=c(1, 2), memp=memp)
M7.MME.pareto[i,]=MME.pareto$estimate
#trgamma
M8.MLE.trgamma[i,]=mledist(Sample1.list[[i]], "trgamma")$estimate
memp  <-  function(x, order) mean(x^order)
MME.trgamma=mmedist(Sample1.list[[i]], "trgamma", order=c(1, 2, 3), memp=memp)
M8.MME.trgamma[i,]=MME.trgamma$estimate
}
```
Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **lnorm** family) as follows:

```{r Tabxx1S1, echo=T, comment = NA}
#print table
kable(cbind(M1.MLE.lnorm, M1.MME.lnorm), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **exp** family) as follows:
```{r Tabxx2S1, echo=T, comment = NA}
#print table
kable(cbind(M2.MLE.exp, M2.MME.exp), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **gamma** family) as follows:
```{r Tabxx3S1, echo=T, comment = NA}
#print table
kable(cbind(M3.MLE.gamma, M3.MME.gamma), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **weibull** family) as follows:
```{r Tabxx4S1, echo=T, comment = NA}
#print table
kable(cbind(M4.MLE.weibull, M4.MME.weibull), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **llogis** family) as follows:
```{r Tabxx5S1, echo=T, comment = NA}
#print table
kable(cbind(M5.MLE.llogis, M5.MME.llogis), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **invweibull** family) as follows:
```{r Tabxx6S1, echo=T, comment = NA}
#print table
kable(cbind(M6.MLE.invweibull, M6.MME.invweibull), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **pareto** family) as follows:
```{r Tabxx7S1, echo=T, comment = NA}
#print table
kable(cbind(M7.MLE.pareto, M7.MME.pareto), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

Sample 1 (simulated from *LogNormal*) of sizes $n=\{25, 50, 100, 250, 500\}$ are fitted via *MLE* and *MOM* (assuming **trgamma** family) as follows:
```{r Tabxx8S1, echo=T, comment = NA}
#print table
kable(cbind(M8.MLE.trgamma, M8.MME.trgamma), digits = 7, format.args = list(big.mark = ","), caption = "MLE estimates (left) and MOM estimates (right)")
```

There is a lot to absorb from these estimates, and we will not analyse every single set of estimates, but we note the following:

i) **MLE** and **MOM** estimates are identical when fitting **exp**, as expected since both should equal to $1/m_1$ where $m_1$ is the sample mean estimator;
ii) There are no obvious concerns with almost all **MLE** estimates though **pareto** family when $n=100$ are very different than all other samples;
iii) There are no obvious concerns with **MOM** estimates when fitting **lnorm**, **exp** and **gamma** families; 
iv) As in Section \@ref(MOM), there is no doubt that **MOM** fails when fitting **llogis** and **trgamma** families;
v) As in Section \@ref(MOM), **MOM** estimates when fitting **weibull**, **invweibull** and **pareto** families should be checked and bespoke remedies might be needed; **MOM**(like **MLE**) estimates for **pareto** family when $n=100$ are very different than all other samples.

Let us check the **MOM** estimates for **pareto** family when $n=100$, though the **MLE** counterpart has the same issue.  
```{r}
xxx <- c(mean(Sample1.list[[3]]),mean(Sample1.list[[3]]^2))
xxxP<-c(mpareto(1, shape=M7.MME.pareto[3,1], scale = M7.MME.pareto[3,2]),mpareto(1, shape=M7.MME.pareto[3,1], scale = M7.MME.pareto[3,2]))
cat("First two (raw) moments of Samples 1 with n=100 -- empirical estimates")
xxx
cat("Pareto MOM estimates of Samples 1 with n=100 -- (a=shape, b=scale)")
xxxP
cat("The diferences between the empirical and theoretical MOM for weibull")
xxx-xxxP
```
Thus, we need to find a bespoke remedy for this issue. One could help R to better estimate the **pareto** MOM estimates from first principles.  That is, we have to solve the following system of equations:
$$\mathbb{E}[X]=\frac{\lambda}{\alpha-1}=m_1\quad\text{and}\quad\mathbb{E}\big[X^2\big]=\frac{2\lambda^2}{(\alpha-1)(\alpha-2)}=m_2,$$
where $m_1$ and $m_2$ are the first and second empirical moment estimates. Recall that the shape parameter should satisfy $\alpha>2$ so that the first two moments are finite, and $\lambda>0$ should always need to hold, since our **pareto** parametrisation has the following cumulative distribution function:
$$
F_{Pareto}(x;\alpha,\lambda)=\Pr(X\le x)=1-\left(\frac{\lambda}{\lambda+x}\right)^\alpha\quad\text{for all}\;x\ge0\quad\text{with}\;\alpha,\lambda>0.
$$
Our system of equations requires that $\lambda=m_1(\alpha-1)$, which in turn gives that
$$
m_2=2m_1^2\cdot\frac{\alpha-1}{\alpha-2}\implies \hat{\alpha}_{MOM}=2+\left(\frac{m_2}{2m_1^2}-1\right)^{-1}\quad\text{and}\quad \hat{\lambda}_{MOM}=m_1(\hat{\alpha}_{MOM}-1).
$$
Recall that $\hat{\alpha}_{MOM}>2$ and $\hat{\lambda}_{MOM}>0$ should hold, which is true as long as $\frac{m_2}{2m_1^2}-1>0$, but for this particular sample $\frac{m_2}{2m_1^2}-1=1.035436$ so the **MOM** are feasible.  We could now check whether we got what we need not only what we hoped to: 
```{r}
xxx <- c(mean(Sample1.list[[3]]),mean(Sample1.list[[3]]^2))
alpha1MOM <- 2+(xxx[2]/(2*xxx[1]^2)-1)^(-1)
lambda1MOM <- (alpha1MOM-1)*xxx[1]
cat("Pareto MOM estimates of Samples 1 with n=100 -- (alpha1=shape, lambda1=scale)")
c(alpha1MOM,lambda1MOM)
xxxP1<-c(mpareto(1, shape=alpha1MOM, scale = lambda1MOM),mpareto(2, shape=alpha1MOM, scale = lambda1MOM))
cat("First two (raw) moments of Samples 1 with n=100 -- empirical estimates")
xxx
cat("First two (raw) moments of Samples 1 with n=100 -- theoretical MOM pareto")
xxxP1
cat("The diferences between the empirical and theoretical MOM for pareto")
xxx-xxxP1
```
and we notice that the empirical and theoretical moments are extremely close to each other with an error up to $10^{-5}$. These **MOM** estimates for *pareto* will not be further used so that the R code does not get more cumbersome that is now, but keep in mind that some **MOM** sets of estimates are not very accurate, particularly those **MOM** estimates when fitting **weibull**, **invweibull** and **pareto** families.


Q-Q plots and P-P plots are intuitive graphical tools for checking two given distributions. Q-Q plots tend to be more useful, but for completeness, all plots are provided as follows:

i) Compare *MLE* and *MOM* fitted distribution (by assuming *lnorm*) with the empirical distribution -- see the *Q-Q Plots* from Figure \@ref(fig:QQlnorm);
ii) Compare *MLE* and *MOM* fitted distribution by assuming *lnorm*) with the empirical distribution  -- see the *P-P Plots* from Figure \@ref(fig:PPlnorm);
iii) Compare *MLE* and *MOM* fitted distribution (by assuming *exp*) with the empirical distribution  -- see the *Q-Q Plots* from Figure \@ref(fig:QQexp);
iv) Compare *MLE* and *MOM* fitted distribution (by assuming *exp*) with the empirical distribution  -- see the *P-P Plots* from Figure \@ref(fig:PPexp).

Note that the five samples (of sizes $n=\{25, 50, 100, 250, 500\}$) are drawn/simulated from *LogNormal* , i.e. the **true** distribution is *LogNormal*, but these samples are fitted via *MLE/MOM* by assuming *lnorm* and *exp* families. Clearly, the other six parametric assumptions could be checked in the same fashion.


```{r QQlnorm, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=6, fig.cap = 'Q-Q plots (by assuming lnorm) for MLE (top) and MOM (bottom)'} 
par(mfrow = c(2, 5))
mle.fln <- fitdist(Sample1.list[[1]], "lnorm", method="mle")
qqcomp(list(mle.fln), legendtext = "lnorm (mle, n=25)")
mle.fln <- fitdist(Sample1.list[[2]], "lnorm", method="mle")
qqcomp(list(mle.fln), legendtext = "lnorm (mle, n=50)")
mle.fln <- fitdist(Sample1.list[[3]], "lnorm", method="mle")
qqcomp(list(mle.fln), legendtext = "lnorm (mle, n=100)")
mle.fln <- fitdist(Sample1.list[[4]], "lnorm", method="mle")
qqcomp(list(mle.fln), legendtext = "lnorm (mle, n=250)")
mle.fln <- fitdist(Sample1.list[[5]], "lnorm", method="mle")
qqcomp(list(mle.fln), legendtext = "lnorm (mle, n=500)")
mme.fln <- fitdist(Sample1.list[[1]], "lnorm", method="mme")
qqcomp(list(mme.fln), legendtext = "lnorm (mme, n=25)")
mme.fln <- fitdist(Sample1.list[[2]], "lnorm", method="mme")
qqcomp(list(mme.fln), legendtext = "lnorm (mme, n=50)")
mme.fln <- fitdist(Sample1.list[[3]], "lnorm", method="mme")
qqcomp(list(mme.fln), legendtext = "lnorm (mme, n=100)")
mme.fln <- fitdist(Sample1.list[[4]], "lnorm", method="mme")
qqcomp(list(mme.fln), legendtext = "lnorm (mme, n=250)")
mme.fln <- fitdist(Sample1.list[[5]], "lnorm", method="mme")
qqcomp(list(mme.fln), legendtext = "lnorm (mme, n=500)")
```


```{r PPlnorm, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=6, fig.cap = 'P-P plots (by assuming lnorm) for MLE (top) and MOM (bottom)'}
par(mfrow = c(2, 5))
mle.fln <- fitdist(Sample1.list[[1]], "lnorm", method="mle")
ppcomp(list(mle.fln), legendtext = "lnorm (mle, n=25)")
mle.fln <- fitdist(Sample1.list[[2]], "lnorm", method="mle")
ppcomp(list(mle.fln), legendtext = "lnorm (mle, n=50)")
mle.fln <- fitdist(Sample1.list[[3]], "lnorm", method="mle")
ppcomp(list(mle.fln), legendtext = "lnorm (mle, n=100)")
mle.fln <- fitdist(Sample1.list[[4]], "lnorm", method="mle")
ppcomp(list(mle.fln), legendtext = "lnorm (mle, n=250)")
mle.fln <- fitdist(Sample1.list[[5]], "lnorm", method="mle")
ppcomp(list(mle.fln), legendtext = "lnorm (mle, n=500)")
mme.fln <- fitdist(Sample1.list[[1]], "lnorm", method="mme")
ppcomp(list(mme.fln), legendtext = "lnorm (mme, n=25)")
mme.fln <- fitdist(Sample1.list[[2]], "lnorm", method="mme")
ppcomp(list(mme.fln), legendtext = "lnorm (mme, n=50)")
mme.fln <- fitdist(Sample1.list[[3]], "lnorm", method="mme")
ppcomp(list(mme.fln), legendtext = "lnorm (mme, n=100)")
mme.fln <- fitdist(Sample1.list[[4]], "lnorm", method="mme")
ppcomp(list(mme.fln), legendtext = "lnorm (mme, n=250)")
mme.fln <- fitdist(Sample1.list[[5]], "lnorm", method="mme")
ppcomp(list(mme.fln), legendtext = "lnorm (mme, n=500)")
```

```{r QQexp, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=6, fig.cap = 'Q-Q plots (by assuming exp) for MLE (top) and MOM (bottom)'}
#Here, in order to use fitdist of exp, we re-scale of the data sample (just here only)
set.seed(1234) #Generating Sample1 and saved in a list
n=c(25, 50, 100, 250, 500)
lnorm.meanlog=10
lnorm.sdlog=1
Sample1.list.rescaled=list(n25=rep(0,25),n50=rep(0,50),n100=rep(0,100),n250=rep(0,250),n500=rep(0,500))
for (i in 1:length(n)) {
Sample1=rlnorm(n[i], meanlog=lnorm.meanlog, sdlog=lnorm.sdlog)
Sample1.list.rescaled[[i]]=Sample1/1000
}
par(mfrow = c(2, 5))
mle.fe <- fitdist(Sample1.list.rescaled[[1]], "exp", method="mle")
qqcomp(list(mle.fe), legendtext = "exp (mle, n=25)")
mle.fe <- fitdist(Sample1.list.rescaled[[2]], "exp", method="mle")
qqcomp(list(mle.fe), legendtext = "exp (mle, n=50)")
mle.fe <- fitdist(Sample1.list.rescaled[[3]], "exp", method="mle")
qqcomp(list(mle.fe), legendtext = "exp (mle, n=100)")
mle.fe <- fitdist(Sample1.list.rescaled[[4]], "exp", method="mle")
qqcomp(list(mle.fe), legendtext = "exp (mle, n=250)")
mle.fe <- fitdist(Sample1.list.rescaled[[5]], "exp", method="mle")
qqcomp(list(mle.fe), legendtext = "exp (mle, n=500)")
mme.fe <- fitdist(Sample1.list.rescaled[[1]], "exp", method="mme")
qqcomp(list(mme.fe), legendtext = "exp (mme, n=25)")
mme.fe <- fitdist(Sample1.list.rescaled[[2]], "exp", method="mme")
qqcomp(list(mme.fe), legendtext = "exp (mme, n=50)")
mme.fe <- fitdist(Sample1.list.rescaled[[3]], "exp", method="mme")
qqcomp(list(mme.fe), legendtext = "exp (mme, n=100)")
mme.fe <- fitdist(Sample1.list.rescaled[[4]], "exp", method="mme")
qqcomp(list(mme.fe), legendtext = "exp (mme, n=250)")
mme.fe <- fitdist(Sample1.list.rescaled[[5]], "exp", method="mme")
qqcomp(list(mme.fe), legendtext = "exp (mme, n=500)")
```


```{r PPexp, comment="", message = FALSE, warning = FALSE, fig.align="center",  fig.width=12, fig.height=6, fig.cap = 'P-P plots (by assuming exp) for MLE (top) and MOM (bottom)'}
par(mfrow = c(2, 5))
mle.fe <- fitdist(Sample1.list.rescaled[[1]], "exp", method="mle")
ppcomp(list(mle.fe), legendtext = "exp (mle, n=25)")
mle.fe <- fitdist(Sample1.list.rescaled[[2]], "exp", method="mle")
ppcomp(list(mle.fe), legendtext = "exp (mle, n=50)")
mle.fe <- fitdist(Sample1.list.rescaled[[3]], "exp", method="mle")
ppcomp(list(mle.fe), legendtext = "exp (mle, n=100)")
mle.fe <- fitdist(Sample1.list.rescaled[[4]], "exp", method="mle")
ppcomp(list(mle.fe), legendtext = "exp (mle, n=250)")
mle.fe <- fitdist(Sample1.list.rescaled[[5]], "exp", method="mle")
ppcomp(list(mle.fe), legendtext = "exp (mle, n=500)")
mme.fe <- fitdist(Sample1.list.rescaled[[1]], "exp", method="mme")
ppcomp(list(mme.fe), legendtext = "exp (mme, n=25)")
mme.fe <- fitdist(Sample1.list.rescaled[[2]], "exp", method="mme")
ppcomp(list(mme.fe), legendtext = "exp (mme, n=50)")
mme.fe <- fitdist(Sample1.list.rescaled[[3]], "exp", method="mme")
ppcomp(list(mme.fe), legendtext = "exp (mme, n=100)")
mme.fe <- fitdist(Sample1.list.rescaled[[4]], "exp", method="mme")
ppcomp(list(mme.fe), legendtext = "exp (mme, n=250)")
mme.fe <- fitdist(Sample1.list.rescaled[[5]], "exp", method="mme")
ppcomp(list(mme.fe), legendtext = "exp (mme, n=500)")
```

Quantiles play an important role in fitting the best possible distribution. In addition, the real data analysis from Section \@ref(RDA) requires estimating quantiles at level $\{50\%, 75\%,95\%\}$. Therefore, we next estimate the same quantiles so that we understand the estimation error in a controlled setting where data are simulated. 

The following quantile related computations are tabulated: i) `True' quantile values and ii) *MLE* and *MOM* and quantile estimates with various sample sizes. 

```{r TrueQTable, echo=T, comment = NA, warning=FALSE}
Quantile.true=qlnorm(c(0.5,0.75,0.95), meanlog=lnorm.meanlog, sdlog=lnorm.sdlog, lower.tail = TRUE, log.p = FALSE)
Quantile.true=matrix(Quantile.true,1,3,dimnames=list(c("Quantile value"),c("(50%)","(75%)","(95%)")))
kable(Quantile.true, digits = 2, format.args = list(big.mark = ","), caption="Quantile values of the true LogNormal distribution at levels 50%, 75% and 95%")
```

```{r EstQTable25, echo=T, comment = NA, warning=FALSE}
suppressWarnings(library(stats))
Table1.1=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","weibull","llogis","invwei","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n25 <- fitdist(Sample1.list[[1]], "lnorm", method="mle")
Quantile.lnorm.mle.n25=quantile(mle.fln.n25, probs = c(0.5, 0.75, 0.95))
mme.fln.n25 <- fitdist(Sample1.list[[1]], "lnorm", method="mme")
Quantile.lnorm.mme.n25=quantile(mme.fln.n25, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n25 <- fitdist(Sample1.list.rescaled[[1]], "exp", method="mle") #Use re-scaled value
Quantile.exp.mle.n25=quantile(mle.fe.n25, probs = c(0.5, 0.75, 0.95))
mme.fe.n25 <- fitdist(Sample1.list[[1]], "exp", method="mme")
Quantile.exp.mme.n25=quantile(mme.fe.n25, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n25 <- fitdist(Sample1.list[[1]], "gamma", method="mle",lower=c(0, 0), start=list(shape =1, rate=1))
Quantile.gamma.mle.n25=quantile(mle.fg.n25, probs = c(0.5, 0.75, 0.95))
mme.fg.n25 <- fitdist(Sample1.list[[1]], "gamma", method="mme")
Quantile.gamma.mme.n25=quantile(mme.fg.n25, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n25 <- fitdist(Sample1.list[[1]], "weibull", method="mle")
Quantile.weibull.mle.n25=quantile(mle.fw.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n25 <- fitdist(Sample1.list[[1]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n25=quantile(mme.fw.n25, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n25 <- fitdist(Sample1.list[[1]], "llogis", method="mle")
Quantile.llogis.mle.n25=quantile(mle.fl.n25, probs = c(0.5, 0.75, 0.95))
#mme.fl.n25 <- fitdist(Sample1.list[[1]], "llogis", method="mme", order=c(1, 2), memp=memp)
#Quantile.llogis.mme.n25=quantile(mme.fl.n25, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n25 <- fitdist(Sample1.list[[1]], "invweibull", method="mle")
Quantile.invweibull.mle.n25=quantile(mle.fi.n25, probs = c(0.5, 0.75, 0.95))
mme.fi.n25 <- fitdist(Sample1.list[[1]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n25=quantile(mme.fi.n25, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n25 <- fitdist(Sample1.list[[1]], "pareto", method="mle")
Quantile.pareto.mle.n25=quantile(mle.fp.n25, probs = c(0.5, 0.75, 0.95))
mme.fp.n25 <- fitdist(Sample1.list[[1]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n25=quantile(mme.fp.n25, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n25 <- fitdist(Sample1.list.rescaled[[1]], "trgamma", method="mle",optim.method="SANN") #Use re-scaled
Quantile.trgamma.mle.n25=quantile(mle.ft.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n25 <- fitdist(Sample1.list[[1]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n25=quantile(mme.ft.n25, probs = c(0.5, 0.75, 0.95))
#Table
Table1.1[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n25$quantiles)-Quantile.true)
Table1.1[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n25$quantiles)-Quantile.true)
Table1.1[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n25$quantiles)*1000-Quantile.true) #Use re-scaled
Table1.1[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n25$quantiles)-Quantile.true)
Table1.1[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n25$quantiles)-Quantile.true)
Table1.1[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n25$quantiles)-Quantile.true)
Table1.1[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n25$quantiles)-Quantile.true)
Table1.1[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n25$quantiles)-Quantile.true)
Table1.1[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n25$quantiles)-Quantile.true)
#Table1.1[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.1[5,c(1,3,5)]=c(NA,NA,NA)
Table1.1[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
#Table1.1[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.1[6,c(1,3,5)]=c(NA,NA,NA)
Table1.1[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n25$quantiles)-Quantile.true)
Table1.1[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n25$quantiles)-Quantile.true)
Table1.1[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n25$quantiles)*1000-Quantile.true) #Use re-scaled
Table1.1[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n25$quantiles)-Quantile.true)
#print table
kable(Table1.1, digits = 3, format.args = list(big.mark = ","), caption="MLE and MOM quantile estimation errors at levels 50%, 75% and 95% for the LogNormal sample of size n=25")
```


```{r EstQTable50, echo=T, comment = NA, warning=FALSE}
Table1.2=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n50 <- fitdist(Sample1.list[[2]], "lnorm", method="mle")
Quantile.lnorm.mle.n50=quantile(mle.fln.n50, probs = c(0.5, 0.75, 0.95))
mme.fln.n50 <- fitdist(Sample1.list[[2]], "lnorm", method="mme")
Quantile.lnorm.mme.n50=quantile(mme.fln.n50, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n50 <- fitdist(Sample1.list.rescaled[[2]], "exp", method="mle") #Use re-scaled
Quantile.exp.mle.n50=quantile(mle.fe.n50, probs = c(0.5, 0.75, 0.95))
mme.fe.n50 <- fitdist(Sample1.list[[2]], "exp", method="mme")
Quantile.exp.mme.n50=quantile(mme.fe.n50, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n50 <- fitdist(Sample1.list[[2]], "gamma", method="mle",lower=c(0, 0), start=list(shape =1, rate=1))
Quantile.gamma.mle.n50=quantile(mle.fg.n50, probs = c(0.5, 0.75, 0.95))
mme.fg.n50 <- fitdist(Sample1.list[[2]], "gamma", method="mme")
Quantile.gamma.mme.n50=quantile(mme.fg.n50, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n50 <- fitdist(Sample1.list[[2]], "weibull", method="mle")
Quantile.weibull.mle.n50=quantile(mle.fw.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n50 <- fitdist(Sample1.list[[2]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n50=quantile(mme.fw.n50, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n50 <- fitdist(Sample1.list[[2]], "llogis", method="mle")
Quantile.llogis.mle.n50=quantile(mle.fl.n50, probs = c(0.5, 0.75, 0.95))
mme.fl.n50 <- fitdist(Sample1.list[[2]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n50=quantile(mme.fl.n50, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n50 <- fitdist(Sample1.list[[2]], "invweibull", method="mle")
Quantile.invweibull.mle.n50=quantile(mle.fi.n50, probs = c(0.5, 0.75, 0.95))
mme.fi.n50 <- fitdist(Sample1.list[[2]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n50=quantile(mme.fi.n50, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n50 <- fitdist(Sample1.list[[2]], "pareto", method="mle")
Quantile.pareto.mle.n50=quantile(mle.fp.n50, probs = c(0.5, 0.75, 0.95))
mme.fp.n50 <- fitdist(Sample1.list[[2]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n50=quantile(mme.fp.n50, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n50 <- fitdist(Sample1.list.rescaled[[2]], "trgamma", method="mle",optim.method="SANN") #Use re-scaled
Quantile.trgamma.mle.n50=quantile(mle.ft.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n50 <- fitdist(Sample1.list[[2]], "trgamma", method="mme",optim.method="SANN", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n50=quantile(mme.ft.n50, probs = c(0.5, 0.75, 0.95))
#Table
Table1.2[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n50$quantiles)-Quantile.true)
Table1.2[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n50$quantiles)-Quantile.true)
Table1.2[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n50$quantiles)*1000-Quantile.true) #Use re-scaled
Table1.2[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n50$quantiles)-Quantile.true)
Table1.2[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n50$quantiles)-Quantile.true)
Table1.2[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n50$quantiles)-Quantile.true)
Table1.2[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n50$quantiles)-Quantile.true)
Table1.2[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n50$quantiles)-Quantile.true)
Table1.2[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n50$quantiles)-Quantile.true)
#Table1.2[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.2[5,c(1,3,5)]=c(NA,NA,NA)
Table1.2[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
#Table1.2[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.2[6,c(1,3,5)]=c(NA,NA,NA)
Table1.2[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n50$quantiles)-Quantile.true)
Table1.2[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n50$quantiles)-Quantile.true)
Table1.2[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n50$quantiles)*1000-Quantile.true) #Use re-scaled
Table1.2[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n50$quantiles)-Quantile.true)
#print table
kable(Table1.2, digits = 3, format.args = list(big.mark = ","), caption="MLE and MOM quantile estimation errors at levels 50%, 75% and 95% for the LogNormal sample of size n=50")
```


```{r EstQTable100, echo=T, comment = NA, warning=FALSE}
Table1.3=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n100 <- fitdist(Sample1.list[[3]], "lnorm", method="mle")
Quantile.lnorm.mle.n100=quantile(mle.fln.n100, probs = c(0.5, 0.75, 0.95))
mme.fln.n100 <- fitdist(Sample1.list[[3]], "lnorm", method="mme")
Quantile.lnorm.mme.n100=quantile(mme.fln.n100, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n100 <- fitdist(Sample1.list.rescaled[[3]], "exp", method="mle")
Quantile.exp.mle.n100=quantile(mle.fe.n100, probs = c(0.5, 0.75, 0.95))
mme.fe.n100 <- fitdist(Sample1.list[[3]], "exp", method="mme")
Quantile.exp.mme.n100=quantile(mme.fe.n100, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n100 <- fitdist(Sample1.list[[3]], "gamma", method="mle",lower=c(0, 0), start=list(shape =1, rate=1))
Quantile.gamma.mle.n100=quantile(mle.fg.n100, probs = c(0.5, 0.75, 0.95))
mme.fg.n100 <- fitdist(Sample1.list[[3]], "gamma", method="mme")
Quantile.gamma.mme.n100=quantile(mme.fg.n100, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n100 <- fitdist(Sample1.list[[3]], "weibull", method="mle")
Quantile.weibull.mle.n100=quantile(mle.fw.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n100 <- fitdist(Sample1.list[[3]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n100=quantile(mme.fw.n100, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n100 <- fitdist(Sample1.list[[3]], "llogis", method="mle")
Quantile.llogis.mle.n100=quantile(mle.fl.n100, probs = c(0.5, 0.75, 0.95))
mme.fl.n100 <- fitdist(Sample1.list[[3]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n100=quantile(mme.fl.n100, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n100 <- fitdist(Sample1.list[[3]], "invweibull", method="mle")
Quantile.invweibull.mle.n100=quantile(mle.fi.n100, probs = c(0.5, 0.75, 0.95))
mme.fi.n100 <- fitdist(Sample1.list[[3]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n100=quantile(mme.fi.n100, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n100 <- fitdist(Sample1.list[[3]], "pareto", method="mle")
Quantile.pareto.mle.n100=quantile(mle.fp.n100, probs = c(0.5, 0.75, 0.95))
mme.fp.n100 <- fitdist(Sample1.list[[3]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n100=quantile(mme.fp.n100, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n100 <- fitdist(Sample1.list.rescaled[[3]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n100=quantile(mle.ft.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n100 <- fitdist(Sample1.list[[3]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n100=quantile(mme.ft.n100, probs = c(0.5, 0.75, 0.95))
#Table
Table1.3[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n100$quantiles)-Quantile.true)
Table1.3[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n100$quantiles)-Quantile.true)
Table1.3[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n100$quantiles)*1000-Quantile.true)
Table1.3[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n100$quantiles)-Quantile.true)
Table1.3[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n100$quantiles)-Quantile.true)
Table1.3[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n100$quantiles)-Quantile.true)
Table1.3[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n100$quantiles)-Quantile.true)
Table1.3[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n100$quantiles)-Quantile.true)
Table1.3[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n100$quantiles)-Quantile.true)
#Table1.3[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.3[5,c(1,3,5)]=c(NA,NA,NA)
Table1.3[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
#Table1.3[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.3[6,c(1,3,5)]=c(NA,NA,NA)
Table1.3[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n100$quantiles)-Quantile.true)
Table1.3[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n100$quantiles)-Quantile.true)
Table1.3[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n100$quantiles)*1000-Quantile.true)
Table1.3[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n100$quantiles)-Quantile.true)
#print table
kable(Table1.3, digits = 3, format.args = list(big.mark = ","), caption="MLE and MOM quantile estimation errors at levels 50%, 75% and 95% for the LogNormal sample of size n=100")
```


```{r EstQTable250, echo=T, comment = NA, warning=FALSE}
Table1.4=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","weibull","llogis","invwei","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n250 <- fitdist(Sample1.list[[4]], "lnorm", method="mle")
Quantile.lnorm.mle.n250=quantile(mle.fln.n250, probs = c(0.5, 0.75, 0.95))
mme.fln.n250 <- fitdist(Sample1.list[[4]], "lnorm", method="mme")
Quantile.lnorm.mme.n250=quantile(mme.fln.n250, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n250 <- fitdist(Sample1.list.rescaled[[4]], "exp", method="mle")
Quantile.exp.mle.n250=quantile(mle.fe.n250, probs = c(0.5, 0.75, 0.95))
mme.fe.n250 <- fitdist(Sample1.list[[4]], "exp", method="mme")
Quantile.exp.mme.n250=quantile(mme.fe.n250, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n250 <- fitdist(Sample1.list[[4]], "gamma", method="mle",lower=c(0, 0), start=list(shape =1, rate=1))
Quantile.gamma.mle.n250=quantile(mle.fg.n250, probs = c(0.5, 0.75, 0.95))
mme.fg.n250 <- fitdist(Sample1.list[[4]], "gamma", method="mme")
Quantile.gamma.mme.n250=quantile(mme.fg.n250, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n250 <- fitdist(Sample1.list[[4]], "weibull", method="mle")
Quantile.weibull.mle.n250=quantile(mle.fw.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n250 <- fitdist(Sample1.list[[4]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n250=quantile(mme.fw.n250, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n250 <- fitdist(Sample1.list[[4]], "llogis", method="mle")
Quantile.llogis.mle.n250=quantile(mle.fl.n250, probs = c(0.5, 0.75, 0.95))
mme.fl.n250 <- fitdist(Sample1.list[[4]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n250=quantile(mme.fl.n250, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n250 <- fitdist(Sample1.list[[4]], "invweibull", method="mle")
Quantile.invweibull.mle.n250=quantile(mle.fi.n250, probs = c(0.5, 0.75, 0.95))
mme.fi.n250 <- fitdist(Sample1.list[[4]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n250=quantile(mme.fi.n250, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n250 <- fitdist(Sample1.list[[4]], "pareto", method="mle")
Quantile.pareto.mle.n250=quantile(mle.fp.n250, probs = c(0.5, 0.75, 0.95))
mme.fp.n250 <- fitdist(Sample1.list[[4]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n250=quantile(mme.fp.n250, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n250 <- fitdist(Sample1.list.rescaled[[4]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n250=quantile(mle.ft.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n250 <- fitdist(Sample1.list[[4]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n250=quantile(mme.ft.n250, probs = c(0.5, 0.75, 0.95))
#Table
Table1.4[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n250$quantiles)-Quantile.true)
Table1.4[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n250$quantiles)-Quantile.true)
Table1.4[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n250$quantiles)*1000-Quantile.true)
Table1.4[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n250$quantiles)-Quantile.true)
Table1.4[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n250$quantiles)-Quantile.true)
Table1.4[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n250$quantiles)-Quantile.true)
Table1.4[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n250$quantiles)-Quantile.true)
Table1.4[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n250$quantiles)-Quantile.true)
Table1.4[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n250$quantiles)-Quantile.true)
#Table1.4[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.4[5,c(1,3,5)]=c(NA,NA,NA)
Table1.4[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
#Table1.4[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.4[6,c(1,3,5)]=c(NA,NA,NA)
Table1.4[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n250$quantiles)-Quantile.true)
Table1.4[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n250$quantiles)-Quantile.true)
Table1.4[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n250$quantiles)*1000-Quantile.true)
Table1.4[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n250$quantiles)-Quantile.true)
#print table
kable(Table1.4, digits = 3, format.args = list(big.mark = ","), caption="MLE and MOM quantile estimation errors at levels 50%, 75% and 95% for the LogNormal sample of size n=250")
```


```{r EstQTable500, echo=T, comment = NA, warning=FALSE}
Table1.5=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n500 <- fitdist(Sample1.list[[5]], "lnorm", method="mle")
Quantile.lnorm.mle.n500=quantile(mle.fln.n500, probs = c(0.5, 0.75, 0.95))
mme.fln.n500 <- fitdist(Sample1.list[[5]], "lnorm", method="mme")
Quantile.lnorm.mme.n500=quantile(mme.fln.n500, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n500 <- fitdist(Sample1.list.rescaled[[5]], "exp", method="mle")
Quantile.exp.mle.n500=quantile(mle.fe.n500, probs = c(0.5, 0.75, 0.95))
mme.fe.n500 <- fitdist(Sample1.list[[5]], "exp", method="mme")
Quantile.exp.mme.n500=quantile(mme.fe.n500, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n500 <- fitdist(Sample1.list[[5]], "gamma", method="mle",lower=c(0, 0), start=list(shape =1, rate=1))
Quantile.gamma.mle.n500=quantile(mle.fg.n500, probs = c(0.5, 0.75, 0.95))
mme.fg.n500 <- fitdist(Sample1.list[[5]], "gamma", method="mme")
Quantile.gamma.mme.n500=quantile(mme.fg.n500, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n500 <- fitdist(Sample1.list[[5]], "weibull", method="mle")
Quantile.weibull.mle.n500=quantile(mle.fw.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n500 <- fitdist(Sample1.list[[5]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n500=quantile(mme.fw.n500, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n500 <- fitdist(Sample1.list[[5]], "llogis", method="mle")
Quantile.llogis.mle.n500=quantile(mle.fl.n500, probs = c(0.5, 0.75, 0.95))
mme.fl.n500 <- fitdist(Sample1.list[[5]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n500=quantile(mme.fl.n500, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n500 <- fitdist(Sample1.list[[5]], "invweibull", method="mle")
Quantile.invweibull.mle.n500=quantile(mle.fi.n500, probs = c(0.5, 0.75, 0.95))
mme.fi.n500 <- fitdist(Sample1.list[[5]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n500=quantile(mme.fi.n500, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n500 <- fitdist(Sample1.list[[5]], "pareto", method="mle")
Quantile.pareto.mle.n500=quantile(mle.fp.n500, probs = c(0.5, 0.75, 0.95))
mme.fp.n500 <- fitdist(Sample1.list[[5]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n500=quantile(mme.fp.n500, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n500 <- fitdist(Sample1.list.rescaled[[5]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n500=quantile(mle.ft.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n500 <- fitdist(Sample1.list[[5]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n500=quantile(mme.ft.n500, probs = c(0.5, 0.75, 0.95))
#Table
Table1.5[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n500$quantiles)-Quantile.true)
Table1.5[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n500$quantiles)-Quantile.true)
Table1.5[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n500$quantiles)*1000-Quantile.true)
Table1.5[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n500$quantiles)-Quantile.true)
Table1.5[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n500$quantiles)-Quantile.true)
Table1.5[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n500$quantiles)-Quantile.true)
Table1.5[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n500$quantiles)-Quantile.true)
Table1.5[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n500$quantiles)-Quantile.true)
Table1.5[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n500$quantiles)-Quantile.true)
#Table1.5[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.5[5,c(1,3,5)]=c(NA,NA,NA)
Table1.5[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
#Table1.5[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.5[6,c(1,3,5)]=c(NA,NA,NA)
Table1.5[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n500$quantiles)-Quantile.true)
Table1.5[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n500$quantiles)-Quantile.true)
Table1.5[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n500$quantiles)*1000-Quantile.true)
Table1.5[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n500$quantiles)-Quantile.true)
#print table
kable(Table1.5, digits = 3, format.args = list(big.mark = ","), caption="MLE and MOM quantile estimation errors at levels 50%, 75% and 95% for the LogNormal sample of size n=500")
```

*Akaike Information Criterion (AIC)* is useful tool for model selection that applies only to MLE and not to MOM. In other words, the smallest or the lowest value (depending how AIC is defined) indicates the **best MLE-based model choice**; the *lowest AIC* indicates the *best* model in R. From now on, the *best* distribution chosen through AIC is denoted as **DISToo**.

```{r, echo=TRUE, include=FALSE}
Table.AIC=matrix(0,8,5,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC[1,]=c(mle.fln.n25$aic, mle.fln.n50$aic, mle.fln.n100$aic, mle.fln.n250$aic, mle.fln.n500$aic)
#Table.AIC[2,]=c(mle.fe.n25$aic, mle.fe.n50$aic, mle.fe.n100$aic, mle.fe.n250$aic, mle.fe.n500$aic)
Table.AIC[2,]=c(NA, NA, NA, NA, NA)
Table.AIC[3,]=c(mle.fg.n25$aic, mle.fg.n50$aic, mle.fg.n100$aic, mle.fg.n250$aic, mle.fg.n500$aic)
Table.AIC[4,]=c(mle.fw.n25$aic, mle.fw.n50$aic, mle.fw.n100$aic, mle.fw.n250$aic, mle.fw.n500$aic)
Table.AIC[5,]=c(mle.fl.n25$aic, mle.fl.n50$aic, mle.fl.n100$aic, mle.fl.n250$aic, mle.fl.n500$aic)
Table.AIC[6,]=c(mle.fi.n25$aic, mle.fi.n50$aic, mle.fi.n100$aic, mle.fi.n250$aic, mle.fi.n500$aic)
Table.AIC[7,]=c(mle.fp.n25$aic, mle.fp.n50$aic, mle.fp.n100$aic, mle.fp.n250$aic, mle.fp.n500$aic)
#Table.AIC[8,]=c(mle.ft.n25$aic, mle.ft.n50$aic, mle.ft.n100$aic, mle.ft.n250$aic, mle.ft.n500$aic)
Table.AIC[8,]=c(NA, NA, NA, NA, NA)
Table.AIC.min=matrix(0,1,5,dimnames=list(c("Min  (AIC)"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC.min[1,]=c(rownames(Table.AIC)[which(Table.AIC[,1] == min(Table.AIC[,1],na.rm = T))],
                   rownames(Table.AIC)[which(Table.AIC[,2] == min(Table.AIC[,2],na.rm = T))],
                   rownames(Table.AIC)[which(Table.AIC[,3] == min(Table.AIC[,3],na.rm = T))],
                   rownames(Table.AIC)[which(Table.AIC[,4] == min(Table.AIC[,4],na.rm = T))],
                   rownames(Table.AIC)[which(Table.AIC[,5] == min(Table.AIC[,5],na.rm = T))])
```

Since MLE for *exp* and *trgamma* requires re-scalling the data, these two distributions are ignored from the AIC-based comparison, though something more clever could have been done.

```{r AIClogN, echo=T, comment = NA}
#print table
kable(Table.AIC, digits = 3, format.args = list(big.mark = ","), caption="AIC summary for LogNormal samples of size n=25, n=50, n=100, n=250 and n=500")
```

```{r AIClogN1, echo=T, comment = NA}
#print table
kable(Table.AIC.min, caption="Best (or DISToo) distribution chosen through AIC for LogNormal samples of size n=25, n=50, n=100, n=250 and n=500")
```
It looks like AIC does make the right choice, except of the LogNormal sample of size $n=250$, though *llogis* marginally surpasses the `true' choice, namely *lnorm*.

Point estimates -- such our previous quantile estimates -- are sensitive to sampling error, and thus, bootstrapping is often use to reduce the uncertainty of point estimation. That is, we bootstrap $n_b=25$ bootstrap samples with replacement from the corresponding sample and fit only the parametric family chosen before as the *best AIC* distribution, which means that we are

i) Sampling with replacement $n_b=25$ samples of size $n=25$ from our initial LogNormal sample of size $n=25$ and fit all $n_b=25$ samples via *lnorm* MLE;
ii) Sampling with replacement $n_b=25$ samples of size $n=50$ from our initial LogNormal sample of size $n=50$ and fit all $n_b=25$ samples via *lnorm* MLE;
iii) Sampling with replacement $n_b=25$ samples of size $n=100$ from our initial LogNormal sample of size $n=100$ and fit all $n_b=25$ samples via *lnorm* MLE;
iv) Sampling with replacement $n_b=25$ samples of size $n=250$ from our initial LogNormal sample of size $n=250$ and fit all $n_b=25$ samples via *llogis* MLE;
v) Sampling with replacement $n_b=25$ samples of size $n=500$ from our initial LogNormal sample of size $n=500$ and fit all $n_b=25$ samples via *lnorm* MLE;

Recall that AIC identified *lnorm* as the best choice for our initial samples of sizes $n\in\{25,50,100,500\}$, while *llogis* was the best choice for our initial sample of size $n=250$.

The bootstrap sampling for i) to v) -- defined above -- are now given. 

```{r}
#fix the seed inside the loop -- e.g. set.seed(567*b) -- so the bootstrap sampling generates the same output every time the code is run
#Sample1: the case of n=25, **lnorm** based on min(AIC), samples with replacement=25
MLE.lnorm.boot.n25=matrix(0,25,2,dimnames=list(c(1:25),c("meanlog","sdlog")))
for (b in 1:25) {
set.seed(567*b)#!!!set the seed inside the loop!!!
Sample1.boot=sample(Sample1.list[[1]], replace = TRUE)
MLE.lnorm=mledist(Sample1.boot, "lnorm")
MLE.lnorm.boot.n25[b,]=MLE.lnorm$estimate
}
MLE.lnorm.boot.mean.n25=colMeans(MLE.lnorm.boot.n25)
#Sample1: the case of n=50, **lnorm** based on min(AIC), samples with replacement=25
MLE.lnorm.boot.n50=matrix(0,25,2,dimnames=list(c(1:25),c("meanlog","sdlog")))
for (b in 1:25) {
set.seed(567*b)#!!!set the seed inside the loop!!!
Sample1.boot=sample(Sample1.list[[2]], replace = TRUE)
MLE.lnorm=mledist(Sample1.boot, "lnorm")
MLE.lnorm.boot.n50[b,]=MLE.lnorm$estimate
}
MLE.lnorm.boot.mean.n50=colMeans(MLE.lnorm.boot.n50)
#Sample1: the case of n=100, **lnorm** based on min(AIC), samples with replacement=25
MLE.lnorm.boot.n100=matrix(0,25,2,dimnames=list(c(1:25),c("meanlog","sdlog")))
for (b in 1:25) {
set.seed(567*b)#!!!set the seed inside the loop!!!
Sample1.boot=sample(Sample1.list[[3]], replace = TRUE)
MLE.lnorm=mledist(Sample1.boot, "lnorm")
MLE.lnorm.boot.n100[b,]=MLE.lnorm$estimate
}
MLE.lnorm.boot.mean.n100=colMeans(MLE.lnorm.boot.n100)
#Sample1: the case of n=250, **llogis** based on min(AIC), samples with replacement=25
MLE.llogis.boot.n250=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
set.seed(567*b)#!!!set the seed inside the loop!!!
Sample1.boot=sample(Sample1.list[[4]], replace = TRUE)
MLE.llogis=mledist(Sample1.boot, "llogis")
MLE.llogis.boot.n250[b,]=MLE.llogis$estimate
}
MLE.llogis.boot.mean.n250=colMeans(MLE.llogis.boot.n250)
#Sample1: the case of n=500, **lnorm** based on min(AIC), samples with replacement=25
MLE.lnorm.boot.n500=matrix(0,25,2,dimnames=list(c(1:25),c("meanlog","sdlog")))
for (b in 1:25) {
set.seed(567*b)#!!!set the seed inside the loop!!!
Sample1.boot=sample(Sample1.list[[5]], replace = TRUE)
MLE.lnorm=mledist(Sample1.boot, "lnorm")
MLE.lnorm.boot.n500[b,]=MLE.lnorm$estimate
}
MLE.lnorm.boot.mean.n500=colMeans(MLE.lnorm.boot.n500)
```

The sets of $n_b=25$ MLE estimates are now averaged and obtain the so-called *Bootstrap MLE estimates*, which are tabulated below. 


```{r BootEst25, echo=T, comment = NA}
kable(MLE.lnorm.boot.mean.n25, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming lnorm (n=25)")
```
```{r BootEst50, echo=T, comment = NA}
kable(MLE.lnorm.boot.mean.n50, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming lnorm (n=50)")
```

```{r BootEst100, echo=T, comment = NA}
kable(MLE.lnorm.boot.mean.n100, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming lnorm (n=100)")
```

```{r BootEst250, echo=T, comment = NA}
kable(MLE.llogis.boot.mean.n250, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming llogis (n=250)")
```

```{r BootEst500, echo=T, comment = NA}
kable(MLE.lnorm.boot.mean.n500, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming lnorm (n=500)")
```

We now compute the bootstrap quantile estimates as we have agreed upon the parametric model is the 'best' option. Recall that **Dist00** is *lnorm* for all $n=\{25, 50, 100, 500\}$ and *llogis* if $n=250$, while the **true** distribution is *lnorm* of $(\mu=10,\sigma=1)$.

We report the following: 

\begin{eqnarray*} 
&&(1)\;\text{Compute} \;q\big(xx; \hat{\theta}_{MLE}(n,i), Dist00\big) \quad \text{for all}\; i=1,\ldots, 25\quad\text{and}\; n=\{25, 50, 100, 250,  500\},\\
&&(2)\;\text{Compute} \;\bar{\bar{q}}_{boot} (xx;n) =\frac{1}{25} \sum_{i=1}^{25} q\big(xx; \hat{\theta}_{MLE}(n,i), Dist00\big),\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\ 
&&(3)\;\text{Report} \;\big|\bar{\bar{q}}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},
\end{eqnarray*}
where (1) are the quantile estimates for (individual) bootstrap samples, (2) are quantile bootstrap estimates, and (3) evaluate the estimation errors with respect to (2). All computations are reported for the three quantile levels $xx=\{50\%, 75\%, 95\%\}$. 

The background computations -- for (1), (2) and (3) -- are now given. 

```{r, eval=TRUE, echo=TRUE}
#(best n=25) lnorm
Quantile.best.mle.n25.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n25.each[i,]=qlnorm(c(0.5,0.75,0.95), meanlog=MLE.lnorm.boot.n25[i,1], sdlog=MLE.lnorm.boot.n25[i,2])
}
Quantile.best.mle.n25.barbar=colMeans(Quantile.best.mle.n25.each)
#(best n=50) lnorm
Quantile.best.mle.n50.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n50.each[i,]=qlnorm(c(0.5,0.75,0.95), meanlog=MLE.lnorm.boot.n50[i,1], sdlog=MLE.lnorm.boot.n50[i,2])
}
Quantile.best.mle.n50.barbar=colMeans(Quantile.best.mle.n50.each)
#(best n=100) lnorm
Quantile.best.mle.n100.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n100.each[i,]=qlnorm(c(0.5,0.75,0.95), meanlog=MLE.lnorm.boot.n100[i,1], sdlog=MLE.lnorm.boot.n100[i,2])
}
Quantile.best.mle.n100.barbar=colMeans(Quantile.best.mle.n100.each)
#(best n=250) llogis
Quantile.best.mle.n250.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n250.each[i,]=qllogis(c(0.5,0.75,0.95), shape=MLE.llogis.boot.n250[i,1], scale=MLE.llogis.boot.n250[i,2])
}
Quantile.best.mle.n250.barbar=colMeans(Quantile.best.mle.n250.each)
#(best n=500) lnorm
Quantile.best.mle.n500.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n500.each[i,]=qlnorm(c(0.5,0.75,0.95), meanlog=MLE.lnorm.boot.n500[i,1], sdlog=MLE.lnorm.boot.n500[i,2])
}
Quantile.best.mle.n500.barbar=colMeans(Quantile.best.mle.n500.each)
# Table2a
Table2a=matrix(0,5,3,dimnames=list(c("n=25 (lnorm)","n=50 (lnorm)","n=100 (lnorm)","n=250 (llogis)","n=500 (lnorm)"),c("boot(50%)","boot(75%)","boot(95%)")))
Table2a[1,]=abs(Quantile.best.mle.n25.barbar-Quantile.true) #lnorm
Table2a[2,]=abs(Quantile.best.mle.n50.barbar-Quantile.true) #lnorm
Table2a[3,]=abs(Quantile.best.mle.n100.barbar-Quantile.true)#lnorm
Table2a[4,]=abs(Quantile.best.mle.n250.barbar-Quantile.true)#llogis
Table2a[5,]=abs(Quantile.best.mle.n500.barbar-Quantile.true)#lnorm
```

The output (2) is summarised below.

```{r TableBootErr, echo=T, comment = NA}
kable(Table2a, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors for LogNormal samples of size n=25, n=50, n=100, n=250 and n=500")
```

The $n=250$ sample leads to enormous quantile bootstrap estimates at levels $xx=\{75\%, 95\%\}$, and we need to understand why we have such an anomaly.  Recall that MLE estimations in R rely on some optimisation solvers that may not identify a global maximum (if that would exist), but ignore this possible issue once again, and look into the quantile estimates for (individual) bootstrap samples -- see (1) -- when $n=250$.

```{r TableBootEstn250, echo=T, comment = NA}
kable(Quantile.best.mle.n250.each[order(Quantile.best.mle.n250.each[,3]),], digits = 3, format.args = list(big.mark = ","), caption="Quantile estimates for (individual) bootstrap samples coming from the LogNormal sample of size n=250")
```

Note that the peculiar large quantile bootstrap estimates for $n=250$ *(llogis)* is due to some -- three in this case -- extraordinary large quantile estimates for (individual) bootstrap samples amongst the $n_b=25$ estimates. If we exclude these three bootstrap samples, then we could re-compute (2) based the 22 individual bootstrap samples, and we get that
```{r TableBootErrn250NoOutlier, echo=T, comment = NA}
#Check this later
Reorder <- Quantile.best.mle.n250.each[order(Quantile.best.mle.n250.each[,3]),]
Quantile.best.mle.n250.barbar.no.outlier <- colMeans(Reorder[-c(23,24,25),])
No.outlier <- matrix(0,2,3)
No.outlier[1,] <- abs(Quantile.best.mle.n250.barbar.no.outlier-Quantile.true)
No.outlier[2,] <- abs(Quantile.best.mle.n250.barbar-Quantile.true)#llogis; as computed before
colnames(No.outlier) <- c("q_boot(50%)","q_boot(75%)","q_boot(95%)")
rownames(No.outlier) <- c("n=250 (llogis) No outliers (22 bootstrap samples)","n=250 (llogis) With outliers (25 bootstrap samples)")
kable(No.outlier, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors for LogNormal samples of size n=250 without outliers")
```
The estimation errors have been reduced by eliminating the outliers induced by bootstrap sampling error, and the bootstrap estimates are more reasonable. The summary of the quantile bootstrap errors is given below:

```{r TableBootErrSummary, echo=T, comment = NA}
Table2aF <- matrix(0,6,3,dimnames=list(c("n=25 (lnorm)","n=50 (lnorm)","n=100 (lnorm)", "n=250 (llogis) With outliers (25 bootstrap samples)", "n=250 (llogis) No outliers (22 bootstrap samples)", "n=500 (lnorm)"),c("boot(50%)","boot(75%)","boot(95%)")))
Table2aF[1,] <- abs(Quantile.best.mle.n25.barbar-Quantile.true) #lnorm
Table2aF[2,] <- abs(Quantile.best.mle.n50.barbar-Quantile.true) #lnorm
Table2aF[3,] <- abs(Quantile.best.mle.n100.barbar-Quantile.true)#lnorm
Table2aF[4,] <- abs(Quantile.best.mle.n250.barbar-Quantile.true)#llogis
Table2aF[5,] <- No.outlier[1,]#llogis
Table2aF[6,] <- abs(Quantile.best.mle.n500.barbar-Quantile.true)#lnorm
kable(Table2aF, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (3) for all LogNormal samples with and without outliers")
```



So far, we reported the bootstrap errors -- see (3) -- and two additional sets of estimates and their associated estimation errors are provided.  Specifically, we 

\begin{eqnarray*} 
&&(3)\;\text{Report} \;\big|\bar{\bar{q}}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(4)\;\text{Report} \;\big|\bar{q}^{MLE}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(5)\;\text{Report} \;\big|\bar{q}^{MLE}_{sample} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
\end{eqnarray*}
where $\bar{\bar{q}}_{boot} (xx;n)$ is the quantile bootstrap estimator
$$\bar{\bar{q}}_{boot} (xx;n) =\frac{1}{25} \sum_{i=1}^{25} q\big(xx; \hat{\theta}_{MLE}(n,i), Dist00\big),\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},$$
$\bar{q}^{MLE}_{boot} (xx;n)$ is the quantile based on *Dist00* with parameters estimated via the parameter bootstrap estimates
$$\bar{q}^{MLE}_{boot} (xx;n) = q\bigg(xx; \widehat{\Theta}^{boot}_{MLE}(n), Dist00\bigg),\quad\text{where}\;\widehat{\Theta}^{boot}_{MLE}(n)=\frac{1}{25}\sum^{25}_{i=1} \hat{\Theta}^{boot}_{MLE}(n,i),\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},$$
and $\bar{q}^{MLE}_{sample} (xx;n)$ is the quantile based on *Dist00* with parameters estimated via the parameter sample estimates (i.e. without bootstrapping) 
$$\bar{q}^{MLE}_{sample} (xx;n) = q\bigg(xx; \widehat{\Theta}^{sample}_{MLE}(n), Dist00\bigg),\quad\text{where}\;\widehat{\Theta}^{boot}_{MLE}(n)=\frac{1}{25}\sum^{25}_{i=1} \hat{\Theta}^{boot}_{MLE}(n,i),\quad \text{for all}\;n=\{25, 50, 100, 250,  500\}.$$
Note that $\bar{q}^{MLE}_{sample} (xx;n)$ were computed at the beginning of this section introducing the bootstrap computations. All computations are reported for the three quantile levels $xx=\{50\%, 75\%, 95\%\}$. 



```{r, eval=TRUE, echo=TRUE}
Table2b <- matrix(0,5,6,dimnames=list(c("n=25","n=50","n=100","n=250","n=500"),c("boot(50%)","boot(75%)","boot(95%)","table1(50%)","table1(75%)","table1(95%)")))
#Quantile.true
#################n=25, best model: lnorm
Quantile.best.mle.n25 <- qlnorm(c(0.5,0.75,0.95), meanlog = MLE.lnorm.boot.mean.n25[1], sdlog = MLE.lnorm.boot.mean.n25[2]) 
#Quantile.best.mle.n25
Table2b[1,c(1,2,3)] <- abs(Quantile.best.mle.n25-Quantile.true)
#################n=50, best model: lnorm
Quantile.best.mle.n50 <- qlnorm(c(0.5,0.75,0.95), meanlog = MLE.lnorm.boot.mean.n50[1], sdlog = MLE.lnorm.boot.mean.n50[2]) 
#Quantile.best.mle.n50
Table2b[2,c(1,2,3)] <- abs(Quantile.best.mle.n50-Quantile.true)
#################n=100, best model: lnorm
Quantile.best.mle.n100 <- qlnorm(c(0.5,0.75,0.95), meanlog = MLE.lnorm.boot.mean.n100[1], sdlog = MLE.lnorm.boot.mean.n100[2])
#Quantile.best.mle.n100
Table2b[3,c(1,2,3)] <- abs(Quantile.best.mle.n100-Quantile.true)
#################n=250, best model: llogis 
Quantile.best.mle.n250 <- qllogis(c(0.5,0.75,0.95), shape=MLE.llogis.boot.mean.n250[1], scale =MLE.llogis.boot.mean.n250[2])
#Quantile.best.mle.n250
Table2b[4,c(1,2,3)] <- abs(Quantile.best.mle.n250-Quantile.true)
#################n=500, best model: lnorm
Quantile.best.mle.n500 <- qlnorm(c(0.5,0.75,0.95), meanlog = MLE.lnorm.boot.mean.n500[1], sdlog = MLE.lnorm.boot.mean.n500[2])
#Quantile.best.mle.n500
Table2b[5,c(1,2,3)] <- abs(Quantile.best.mle.n500-Quantile.true)
#From Table 1 before
Table2b[1,c(4,5,6)] <- Table1.1[1,c(2,4,6)] #lnorm, i.e first row
Table2b[2,c(4,5,6)] <- Table1.2[1,c(2,4,6)] #inorm, i.e first row
Table2b[3,c(4,5,6)] <- Table1.3[1,c(2,4,6)] #lnorm, i.e first row
Table2b[4,c(4,5,6)] <- Table1.4[5,c(2,4,6)] #llogis, i.e fifth row
Table2b[5,c(4,5,6)] <- Table1.5[1,c(2,4,6)] #lnorm, i.e first row 
colnames(Table2b) <- c("q_boot(50%)","q_boot(75%)","q_boot(95%)", "q_sample(50%)","q_sample(75%)","q_sample(95%)")
rownames(Table2b) <- c("n=25 (lnorm)", "n=50 (lnorm)", "n=100 (lnorm)", "n=250 (llogis)", "n=500 (lnorm)")
```


```{r TableBootEstErrors, echo=T, comment = NA}
kable(Table2b, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (4) and (5) for all LogNormal samples with outliers")
```
The estimation errors show a reasonable pattern, since the larger the sample is the smaller the error is.  Note that outliers are not removed, and in particular sample of size $n=250$ shows a weird behaviour.  This is very likely due to sensitivity of the R optimisation function used in the MLE computations, which escalated the presence of outliers in individual bootstrap samples. Therefore, one should first check the optimality of the MLE estimates computed by R, and then detect possible outliers. 


## Weibull Sample with Moderately Heavy Tail {#S2}

We now redo the computations from Sections \@ref(MLE), \@ref(MOM) and \@ref(S1) for **Sample 2**, which is drawn from the Weibull distribution with parameters $a=0.5$ and $b=1$.  Note that this particular Weibull distribution is a **moderately heavy tailed** distribution since $a<1$. We only report the very final R outputs, since the intermediate steps are very similar to those from **Sample 1** computations. 

Note that *set.seed(1234)* is the seed set for, but no seed is set for bootstrapping computations. 

```{r}
set.seed(1234) #Generating Sample1 and saved in a list
n=c(25, 50, 100, 250, 500)
weibull.shape=0.5
weibull.scale=1    
Sample2.list=list(n25=rep(0,25),n50=rep(0,50),n100=rep(0,100),n250=rep(0,250),n500=rep(0,500))
for (i in 1:length(n)) {
Sample2=rweibull(n[i], shape=weibull.shape , scale=weibull.scale)  
Sample2.list[[i]]=Sample2
}
```
```{r,echo=FALSE, include=FALSE}
#MLE and MME for 8 different distributions and saved in each matrix
M1.MLE.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MLE.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MLE.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MLE.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MLE.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MLE.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MLE.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MLE.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
M1.MME.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MME.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MME.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MME.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MME.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MME.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MME.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MME.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
for (i in 1:length(n)) {
#lnorm
M1.MLE.lnorm[i,]=mledist(Sample2.list[[i]], "lnorm")$estimate
M1.MME.lnorm[i,]=mmedist(Sample2.list[[i]], "lnorm")$estimate
#exp
MLE.exp=mledist(Sample2.list[[i]], "exp")
if (is.na(MLE.exp$estimate)==is.na(NA)) {MLE.exp$estimate=1/mean(Sample2.list[[i]])}
M2.MLE.exp[i,]=MLE.exp$estimate
M2.MME.exp[i,]=mmedist(Sample2.list[[i]], "exp")$estimate
#gamma
MLE.gamma=mledist(Sample2.list[[i]], "gamma")  #when the meanlog is too large, become NA
M3.MLE.gamma[i,]=MLE.gamma$estimate
if (is.na(MLE.gamma$estimate[1])==is.na(NA)) {
  MLE.gamma$estimate=gammaMLE(Sample2.list[[i]])
  MLE.gamma=c(MLE.gamma$estimate[1],1/MLE.gamma$estimate[2])
  M3.MLE.gamma[i,]=MLE.gamma}
M3.MME.gamma[i,]=mmedist(Sample2.list[[i]], "gamma")$estimate
#weibull
M4.MLE.weibull[i,]=mledist(Sample2.list[[i]], "weibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.weibull=mmedist(Sample2.list[[i]], "weibull", order=c(1, 2), memp=memp)
M4.MME.weibull[i,]=MME.weibull$estimate
#llogis
M5.MLE.llogis[i,]=mledist(Sample2.list[[i]], "llogis")$estimate
memp  <-  function(x, order) mean(x^order)
MME.llogis=mmedist(Sample2.list[[i]], "llogis", order=c(1, 2), memp=memp)
M5.MME.llogis[i,]=MME.llogis$estimate
#invweibull  
M6.MLE.invweibull[i,]=MLE.invweibull=mledist(Sample2.list[[i]], "invweibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.invweibull=mmedist(Sample2.list[[i]], "invweibull", order=c(1, 2), memp=memp)
M6.MME.invweibull[i,]=MME.invweibull$estimate
#pareto
M7.MLE.pareto[i,]=MLE.pareto=mledist(Sample2.list[[i]], "pareto")$estimate
memp  <-  function(x, order) mean(x^order)
MME.pareto=mmedist(Sample2.list[[i]], "pareto", order=c(1, 2), memp=memp)
M7.MME.pareto[i,]=MME.pareto$estimate
#trgamma
M8.MLE.trgamma[i,]=mledist(Sample2.list[[i]], "trgamma")$estimate
memp  <-  function(x, order) mean(x^order)
MME.trgamma=mmedist(Sample2.list[[i]], "trgamma", order=c(1, 2, 3), memp=memp)
M8.MME.trgamma[i,]=MME.trgamma$estimate
}
```


```{r, echo=FALSE, include=FALSE}
#Quantile of the true distribution
Quantile.true=qweibull(c(0.5,0.75,0.95), shape=weibull.shape, scale=weibull.scale, lower.tail = TRUE, log.p = FALSE)
Quantile.true=matrix(Quantile.true,1,3,dimnames=list(c("quantile (true dist)"),c("(50%)","(75%)","(95%)")))
#Quantile.true
```


```{r, echo=FALSE, include=FALSE}
#Table 1.1 (n=25)
Table1.1=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n25 <- fitdist(Sample2.list[[1]], "lnorm", method="mle")
Quantile.lnorm.mle.n25=quantile(mle.fln.n25, probs = c(0.5, 0.75, 0.95))
mme.fln.n25 <- fitdist(Sample2.list[[1]], "lnorm", method="mme")
Quantile.lnorm.mme.n25=quantile(mme.fln.n25, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n25 <- fitdist(Sample2.list[[1]], "exp", method="mle")
Quantile.exp.mle.n25=quantile(mle.fe.n25, probs = c(0.5, 0.75, 0.95))
mme.fe.n25 <- fitdist(Sample2.list[[1]], "exp", method="mme")
Quantile.exp.mme.n25=quantile(mme.fe.n25, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n25 <- fitdist(Sample2.list[[1]], "gamma", method="mle")
Quantile.gamma.mle.n25=quantile(mle.fg.n25, probs = c(0.5, 0.75, 0.95))
mme.fg.n25 <- fitdist(Sample2.list[[1]], "gamma", method="mme")
Quantile.gamma.mme.n25=quantile(mme.fg.n25, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n25 <- fitdist(Sample2.list[[1]], "weibull", method="mle")
Quantile.weibull.mle.n25=quantile(mle.fw.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n25 <- fitdist(Sample2.list[[1]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n25=quantile(mme.fw.n25, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n25 <- fitdist(Sample2.list[[1]], "llogis", method="mle")
Quantile.llogis.mle.n25=quantile(mle.fl.n25, probs = c(0.5, 0.75, 0.95))
mme.fl.n25 <- fitdist(Sample2.list[[1]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n25=quantile(mme.fl.n25, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n25 <- fitdist(Sample2.list[[1]], "invweibull", method="mle")
Quantile.invweibull.mle.n25=quantile(mle.fi.n25, probs = c(0.5, 0.75, 0.95))
mme.fi.n25 <- fitdist(Sample2.list[[1]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n25=quantile(mme.fi.n25, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n25 <- fitdist(Sample2.list[[1]], "pareto", method="mle")
Quantile.pareto.mle.n25=quantile(mle.fp.n25, probs = c(0.5, 0.75, 0.95))
mme.fp.n25 <- fitdist(Sample2.list[[1]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n25=quantile(mme.fp.n25, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n25 <- fitdist(Sample2.list[[1]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n25=quantile(mle.ft.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n25 <- fitdist(Sample2.list[[1]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n25=quantile(mme.ft.n25, probs = c(0.5, 0.75, 0.95))
#Table
Table1.1[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n25$quantiles)-Quantile.true)
Table1.1[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n25$quantiles)-Quantile.true)
Table1.1[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n25$quantiles)-Quantile.true)
Table1.1[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n25$quantiles)-Quantile.true)
Table1.1[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n25$quantiles)-Quantile.true)
Table1.1[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n25$quantiles)-Quantile.true)
Table1.1[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n25$quantiles)-Quantile.true)
Table1.1[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n25$quantiles)-Quantile.true)
Table1.1[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n25$quantiles)-Quantile.true)
Table1.1[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.1[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
Table1.1[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
Table1.1[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n25$quantiles)-Quantile.true)
Table1.1[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n25$quantiles)-Quantile.true)
Table1.1[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n25$quantiles)-Quantile.true)
Table1.1[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n25$quantiles)-Quantile.true)
#Table1.1
```


```{r, echo=FALSE, include=FALSE}
#Table 1.2 (n=50)
Table1.2=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n50 <- fitdist(Sample2.list[[2]], "lnorm", method="mle")
Quantile.lnorm.mle.n50=quantile(mle.fln.n50, probs = c(0.5, 0.75, 0.95))
mme.fln.n50 <- fitdist(Sample2.list[[2]], "lnorm", method="mme")
Quantile.lnorm.mme.n50=quantile(mme.fln.n50, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n50 <- fitdist(Sample2.list[[2]], "exp", method="mle")
Quantile.exp.mle.n50=quantile(mle.fe.n50, probs = c(0.5, 0.75, 0.95))
mme.fe.n50 <- fitdist(Sample2.list[[2]], "exp", method="mme")
Quantile.exp.mme.n50=quantile(mme.fe.n50, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n50 <- fitdist(Sample2.list[[2]], "gamma", method="mle")
Quantile.gamma.mle.n50=quantile(mle.fg.n50, probs = c(0.5, 0.75, 0.95))
mme.fg.n50 <- fitdist(Sample2.list[[2]], "gamma", method="mme")
Quantile.gamma.mme.n50=quantile(mme.fg.n50, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n50 <- fitdist(Sample2.list[[2]], "weibull", method="mle")
Quantile.weibull.mle.n50=quantile(mle.fw.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n50 <- fitdist(Sample2.list[[2]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n50=quantile(mme.fw.n50, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n50 <- fitdist(Sample2.list[[2]], "llogis", method="mle")
Quantile.llogis.mle.n50=quantile(mle.fl.n50, probs = c(0.5, 0.75, 0.95))
mme.fl.n50 <- fitdist(Sample2.list[[2]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n50=quantile(mme.fl.n50, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n50 <- fitdist(Sample2.list[[2]], "invweibull", method="mle")
Quantile.invweibull.mle.n50=quantile(mle.fi.n50, probs = c(0.5, 0.75, 0.95))
mme.fi.n50 <- fitdist(Sample2.list[[2]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n50=quantile(mme.fi.n50, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n50 <- fitdist(Sample2.list[[2]], "pareto", method="mle")
Quantile.pareto.mle.n50=quantile(mle.fp.n50, probs = c(0.5, 0.75, 0.95))
mme.fp.n50 <- fitdist(Sample2.list[[2]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n50=quantile(mme.fp.n50, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n50 <- fitdist(Sample2.list[[2]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n50=quantile(mle.ft.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n50 <- fitdist(Sample2.list[[2]], "trgamma", method="mme",optim.method="SANN", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n50=quantile(mme.ft.n50, probs = c(0.5, 0.75, 0.95))
#Table
Table1.2[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n50$quantiles)-Quantile.true)
Table1.2[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n50$quantiles)-Quantile.true)
Table1.2[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n50$quantiles)-Quantile.true)
Table1.2[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n50$quantiles)-Quantile.true)
Table1.2[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n50$quantiles)-Quantile.true)
Table1.2[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n50$quantiles)-Quantile.true)
Table1.2[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n50$quantiles)-Quantile.true)
Table1.2[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n50$quantiles)-Quantile.true)
Table1.2[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n50$quantiles)-Quantile.true)
Table1.2[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n50$quantiles)-Quantile.true)
Table1.2[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n50$quantiles)-Quantile.true)
Table1.2[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n50$quantiles)-Quantile.true)
Table1.2[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n50$quantiles)-Quantile.true)
Table1.2[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n50$quantiles)-Quantile.true)
Table1.2[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n50$quantiles)-Quantile.true)
Table1.2[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n50$quantiles)-Quantile.true)
#Table1.2
```


```{r, echo=FALSE, include=FALSE}
#Table 1.3 (n=100)
Table1.3=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n100 <- fitdist(Sample2.list[[3]], "lnorm", method="mle")
Quantile.lnorm.mle.n100=quantile(mle.fln.n100, probs = c(0.5, 0.75, 0.95))
mme.fln.n100 <- fitdist(Sample2.list[[3]], "lnorm", method="mme")
Quantile.lnorm.mme.n100=quantile(mme.fln.n100, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n100 <- fitdist(Sample2.list[[3]], "exp", method="mle")
Quantile.exp.mle.n100=quantile(mle.fe.n100, probs = c(0.5, 0.75, 0.95))
mme.fe.n100 <- fitdist(Sample2.list[[3]], "exp", method="mme")
Quantile.exp.mme.n100=quantile(mme.fe.n100, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n100 <- fitdist(Sample2.list[[3]], "gamma", method="mle")
Quantile.gamma.mle.n100=quantile(mle.fg.n100, probs = c(0.5, 0.75, 0.95))
mme.fg.n100 <- fitdist(Sample2.list[[3]], "gamma", method="mme")
Quantile.gamma.mme.n100=quantile(mme.fg.n100, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n100 <- fitdist(Sample2.list[[3]], "weibull", method="mle")
Quantile.weibull.mle.n100=quantile(mle.fw.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n100 <- fitdist(Sample2.list[[3]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n100=quantile(mme.fw.n100, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n100 <- fitdist(Sample2.list[[3]], "llogis", method="mle")
Quantile.llogis.mle.n100=quantile(mle.fl.n100, probs = c(0.5, 0.75, 0.95))
mme.fl.n100 <- fitdist(Sample2.list[[3]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n100=quantile(mme.fl.n100, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n100 <- fitdist(Sample2.list[[3]], "invweibull", method="mle")
Quantile.invweibull.mle.n100=quantile(mle.fi.n100, probs = c(0.5, 0.75, 0.95))
mme.fi.n100 <- fitdist(Sample2.list[[3]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n100=quantile(mme.fi.n100, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n100 <- fitdist(Sample2.list[[3]], "pareto", method="mle")
Quantile.pareto.mle.n100=quantile(mle.fp.n100, probs = c(0.5, 0.75, 0.95))
mme.fp.n100 <- fitdist(Sample2.list[[3]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n100=quantile(mme.fp.n100, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n100 <- fitdist(Sample2.list[[3]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n100=quantile(mle.ft.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n100 <- fitdist(Sample2.list[[3]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n100=quantile(mme.ft.n100, probs = c(0.5, 0.75, 0.95))
#Table
Table1.3[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n100$quantiles)-Quantile.true)
Table1.3[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n100$quantiles)-Quantile.true)
Table1.3[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n100$quantiles)-Quantile.true)
Table1.3[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n100$quantiles)-Quantile.true)
Table1.3[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n100$quantiles)-Quantile.true)
Table1.3[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n100$quantiles)-Quantile.true)
Table1.3[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n100$quantiles)-Quantile.true)
Table1.3[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n100$quantiles)-Quantile.true)
Table1.3[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n100$quantiles)-Quantile.true)
Table1.3[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n100$quantiles)-Quantile.true)
Table1.3[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n100$quantiles)-Quantile.true)
Table1.3[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n100$quantiles)-Quantile.true)
Table1.3[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n100$quantiles)-Quantile.true)
Table1.3[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n100$quantiles)-Quantile.true)
Table1.3[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n100$quantiles)-Quantile.true)
Table1.3[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n100$quantiles)-Quantile.true)
#Table1.3
```


```{r, echo=FALSE, include=FALSE}
#Table 1.4 (n=250)
Table1.4=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n250 <- fitdist(Sample2.list[[4]], "lnorm", method="mle")
Quantile.lnorm.mle.n250=quantile(mle.fln.n250, probs = c(0.5, 0.75, 0.95))
mme.fln.n250 <- fitdist(Sample2.list[[4]], "lnorm", method="mme")
Quantile.lnorm.mme.n250=quantile(mme.fln.n250, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n250 <- fitdist(Sample2.list[[4]], "exp", method="mle")
Quantile.exp.mle.n250=quantile(mle.fe.n250, probs = c(0.5, 0.75, 0.95))
mme.fe.n250 <- fitdist(Sample2.list[[4]], "exp", method="mme")
Quantile.exp.mme.n250=quantile(mme.fe.n250, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n250 <- fitdist(Sample2.list[[4]], "gamma", method="mle")
Quantile.gamma.mle.n250=quantile(mle.fg.n250, probs = c(0.5, 0.75, 0.95))
mme.fg.n250 <- fitdist(Sample2.list[[4]], "gamma", method="mme")
Quantile.gamma.mme.n250=quantile(mme.fg.n250, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n250 <- fitdist(Sample2.list[[4]], "weibull", method="mle")
Quantile.weibull.mle.n250=quantile(mle.fw.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n250 <- fitdist(Sample2.list[[4]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n250=quantile(mme.fw.n250, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n250 <- fitdist(Sample2.list[[4]], "llogis", method="mle")
Quantile.llogis.mle.n250=quantile(mle.fl.n250, probs = c(0.5, 0.75, 0.95))
mme.fl.n250 <- fitdist(Sample2.list[[4]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n250=quantile(mme.fl.n250, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n250 <- fitdist(Sample2.list[[4]], "invweibull", method="mle")
Quantile.invweibull.mle.n250=quantile(mle.fi.n250, probs = c(0.5, 0.75, 0.95))
mme.fi.n250 <- fitdist(Sample2.list[[4]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n250=quantile(mme.fi.n250, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n250 <- fitdist(Sample2.list[[4]], "pareto", method="mle")
Quantile.pareto.mle.n250=quantile(mle.fp.n250, probs = c(0.5, 0.75, 0.95))
mme.fp.n250 <- fitdist(Sample2.list[[4]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n250=quantile(mme.fp.n250, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n250 <- fitdist(Sample2.list[[4]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n250=quantile(mle.ft.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n250 <- fitdist(Sample2.list[[4]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n250=quantile(mme.ft.n250, probs = c(0.5, 0.75, 0.95))
#Table
Table1.4[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n250$quantiles)-Quantile.true)
Table1.4[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n250$quantiles)-Quantile.true)
Table1.4[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n250$quantiles)-Quantile.true)
Table1.4[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n250$quantiles)-Quantile.true)
Table1.4[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n250$quantiles)-Quantile.true)
Table1.4[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n250$quantiles)-Quantile.true)
Table1.4[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n250$quantiles)-Quantile.true)
Table1.4[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n250$quantiles)-Quantile.true)
Table1.4[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n250$quantiles)-Quantile.true)
Table1.4[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n250$quantiles)-Quantile.true)
Table1.4[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n250$quantiles)-Quantile.true)
Table1.4[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n250$quantiles)-Quantile.true)
Table1.4[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n250$quantiles)-Quantile.true)
Table1.4[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n250$quantiles)-Quantile.true)
Table1.4[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n250$quantiles)-Quantile.true)
Table1.4[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n250$quantiles)-Quantile.true)
#Table1.4
```

```{r, echo=FALSE, include=FALSE}
#Table 1.5 (n=500)
Table1.5=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n500 <- fitdist(Sample2.list[[5]], "lnorm", method="mle")
Quantile.lnorm.mle.n500=quantile(mle.fln.n500, probs = c(0.5, 0.75, 0.95))
mme.fln.n500 <- fitdist(Sample2.list[[5]], "lnorm", method="mme")
Quantile.lnorm.mme.n500=quantile(mme.fln.n500, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n500 <- fitdist(Sample2.list[[5]], "exp", method="mle")
Quantile.exp.mle.n500=quantile(mle.fe.n500, probs = c(0.5, 0.75, 0.95))
mme.fe.n500 <- fitdist(Sample2.list[[5]], "exp", method="mme")
Quantile.exp.mme.n500=quantile(mme.fe.n500, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n500 <- fitdist(Sample2.list[[5]], "gamma", method="mle")
Quantile.gamma.mle.n500=quantile(mle.fg.n500, probs = c(0.5, 0.75, 0.95))
mme.fg.n500 <- fitdist(Sample2.list[[5]], "gamma", method="mme")
Quantile.gamma.mme.n500=quantile(mme.fg.n500, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n500 <- fitdist(Sample2.list[[5]], "weibull", method="mle")
Quantile.weibull.mle.n500=quantile(mle.fw.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n500 <- fitdist(Sample2.list[[5]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n500=quantile(mme.fw.n500, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n500 <- fitdist(Sample2.list[[5]], "llogis", method="mle")
Quantile.llogis.mle.n500=quantile(mle.fl.n500, probs = c(0.5, 0.75, 0.95))
mme.fl.n500 <- fitdist(Sample2.list[[5]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n500=quantile(mme.fl.n500, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n500 <- fitdist(Sample2.list[[5]], "invweibull", method="mle")
Quantile.invweibull.mle.n500=quantile(mle.fi.n500, probs = c(0.5, 0.75, 0.95))
mme.fi.n500 <- fitdist(Sample2.list[[5]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n500=quantile(mme.fi.n500, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.n500 <- fitdist(Sample2.list[[5]], "pareto", method="mle")
Quantile.pareto.mle.n500=quantile(mle.fp.n500, probs = c(0.5, 0.75, 0.95))
mme.fp.n500 <- fitdist(Sample2.list[[5]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n500=quantile(mme.fp.n500, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n500 <- fitdist(Sample2.list[[5]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n500=quantile(mle.ft.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n500 <- fitdist(Sample2.list[[5]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n500=quantile(mme.ft.n500, probs = c(0.5, 0.75, 0.95))
#Table
Table1.5[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n500$quantiles)-Quantile.true)
Table1.5[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n500$quantiles)-Quantile.true)
Table1.5[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n500$quantiles)-Quantile.true)
Table1.5[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n500$quantiles)-Quantile.true)
Table1.5[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n500$quantiles)-Quantile.true)
Table1.5[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n500$quantiles)-Quantile.true)
Table1.5[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n500$quantiles)-Quantile.true)
Table1.5[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n500$quantiles)-Quantile.true)
Table1.5[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n500$quantiles)-Quantile.true)
Table1.5[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n500$quantiles)-Quantile.true)
Table1.5[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n500$quantiles)-Quantile.true)
Table1.5[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n500$quantiles)-Quantile.true)
Table1.5[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n500$quantiles)-Quantile.true)
Table1.5[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n500$quantiles)-Quantile.true)
Table1.5[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n500$quantiles)-Quantile.true)
Table1.5[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n500$quantiles)-Quantile.true)
#Table1.5
```

```{r, echo=FALSE, include=FALSE}
Table.AIC=matrix(0,8,5,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC[1,]=c(mle.fln.n25$aic, mle.fln.n50$aic, mle.fln.n100$aic, mle.fln.n250$aic, mle.fln.n500$aic)
Table.AIC[2,]=c(mle.fe.n25$aic, mle.fe.n50$aic, mle.fe.n100$aic, mle.fe.n250$aic, mle.fe.n500$aic)
Table.AIC[3,]=c(mle.fg.n25$aic, mle.fg.n50$aic, mle.fg.n100$aic, mle.fg.n250$aic, mle.fg.n500$aic)
Table.AIC[4,]=c(mle.fw.n25$aic, mle.fw.n50$aic, mle.fw.n100$aic, mle.fw.n250$aic, mle.fw.n500$aic)
Table.AIC[5,]=c(mle.fl.n25$aic, mle.fl.n50$aic, mle.fl.n100$aic, mle.fl.n250$aic, mle.fl.n500$aic)
Table.AIC[6,]=c(mle.fi.n25$aic, mle.fi.n50$aic, mle.fi.n100$aic, mle.fi.n250$aic, mle.fi.n500$aic)
Table.AIC[7,]=c(mle.fp.n25$aic, mle.fp.n50$aic, mle.fp.n100$aic, mle.fp.n250$aic, mle.fp.n500$aic)
Table.AIC[8,]=c(mle.ft.n25$aic, mle.ft.n50$aic, mle.ft.n100$aic, mle.ft.n250$aic, mle.ft.n500$aic)
Table.AIC.min=matrix(0,1,5,dimnames=list(c("Min  (AIC)"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC.min[1,]=c(rownames(Table.AIC)[which(Table.AIC[,1] == min(Table.AIC[,1]))],
                   rownames(Table.AIC)[which(Table.AIC[,2] == min(Table.AIC[,2]))],
                   rownames(Table.AIC)[which(Table.AIC[,3] == min(Table.AIC[,3]))],
                   rownames(Table.AIC)[which(Table.AIC[,4] == min(Table.AIC[,4]))],
                   rownames(Table.AIC)[which(Table.AIC[,5] == min(Table.AIC[,5]))])
```


```{r AICS2, echo=T, comment = NA}
kable(Table.AIC, digits = 3, format.args = list(big.mark = ","), caption=("AIC summary for moderately heavy tailed Weibull samples of size n=25, n=50, n=100, n=250 and n=500"))
```

```{r AICS2min, echo=T, comment = NA}
kable(Table.AIC.min, digits = 3, format.args = list(big.mark = ","), caption=("Best (or DISToo) distribution chosen through AIC for moderately heavy tailed Weibull samples of size n=25, n=50, n=100, n=250 and n=500"))
```
It looks like AIC does make the right choice, except of the Weibull sample of size $n=100$, though *gamma* marginally surpasses the true choice, i.e. *weibull*.

We now perform the usual bootstrap sampling.

```{r, include=FALSE}
#Sample2: the case of n=25, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n25=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample2.boot=sample(Sample2.list[[1]], replace = TRUE)
MLE.weibull=mledist(Sample2.boot, "weibull")
MLE.weibull.boot.n25[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n25=colMeans(MLE.weibull.boot.n25)
```

```{r, include=FALSE}
#Sample2: the case of n=50, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n50=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample2.boot=sample(Sample2.list[[2]], replace = TRUE)
MLE.weibull=mledist(Sample2.boot, "weibull")
MLE.weibull.boot.n50[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n50=colMeans(MLE.weibull.boot.n50)
```

```{r, include=FALSE}
#Sample2: the case of n=100, \textbf{gamma} based on min(AIC), samples with replacement=25
MLE.gamma.boot.n100=matrix(0,25,2,dimnames=list(c(1:25),c("shape","rate")))
for (b in 1:25) {
Sample2.boot=sample(Sample2.list[[3]], replace = TRUE)
MLE.gamma=mledist(Sample2.boot, "gamma")
MLE.gamma.boot.n100[b,]=MLE.gamma$estimate
}
MLE.gamma.boot.mean.n100=colMeans(MLE.gamma.boot.n100)
```

```{r, include=FALSE}
#Sample2: the case of n=250, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n250=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample2.boot=sample(Sample2.list[[4]], replace = TRUE)
MLE.weibull=mledist(Sample2.boot, "weibull")
MLE.weibull.boot.n250[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n250=colMeans(MLE.weibull.boot.n250)
```

```{r, include=FALSE}
#Sample2: the case of n=500, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n500=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample2.boot=sample(Sample2.list[[5]], replace = TRUE)
MLE.weibull=mledist(Sample2.boot, "weibull")
MLE.weibull.boot.n500[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n500=colMeans(MLE.weibull.boot.n500)
```


The sets of $n_b=25$ MLE estimates are now averaged and obtain the so-called *Bootstrap MLE estimates*, which are tabulated below. 


```{r BootEstW25, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n25, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=25)")
```

```{r BootEstW50, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n50, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=50)")
```

```{r BootEstW100, echo=T, comment = NA}
kable(MLE.gamma.boot.mean.n100, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming gamma (n=100)")
```

```{r BootEstW250, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n250, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=250)")
```

```{r BootEstW500, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n500, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=500)")
```

As before, we report three sets of estimation errors as follows:

\begin{eqnarray*} 
&&(3)\;\text{Report} \;\big|\bar{\bar{q}}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(4)\;\text{Report} \;\big|\bar{q}^{MLE}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(5)\;\text{Report} \;\big|\bar{q}^{MLE}_{sample} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\}.
\end{eqnarray*}


```{r, include=FALSE}
#(best n=25) weibull
Quantile.best.mle.n25.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n25.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n25[i,1], scale=MLE.weibull.boot.n25[i,2])
}
Quantile.best.mle.n25.barbar=colMeans(Quantile.best.mle.n25.each)
#(best n=50) weibull
Quantile.best.mle.n50.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n50.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n50[i,1], scale=MLE.weibull.boot.n50[i,2])
}
Quantile.best.mle.n50.barbar=colMeans(Quantile.best.mle.n50.each)
#(best n=100) gamma
Quantile.best.mle.n100.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n100.each[i,]=qgamma(c(0.5,0.75,0.95), shape=MLE.gamma.boot.n100[i,1], rate=MLE.gamma.boot.n100[i,2])
}
Quantile.best.mle.n100.barbar=colMeans(Quantile.best.mle.n100.each)
#(best n=250) weibull
Quantile.best.mle.n250.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n250.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n250[i,1], scale=MLE.weibull.boot.n250[i,2])
}
Quantile.best.mle.n250.barbar=colMeans(Quantile.best.mle.n250.each)
#(best n=500) weibull
Quantile.best.mle.n500.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n500.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n500[i,1], scale=MLE.weibull.boot.n500[i,2])
}
Quantile.best.mle.n500.barbar=colMeans(Quantile.best.mle.n500.each)
# Table2a
Table2a=matrix(0,5,3,dimnames=list(c("n=25 (weibull)","n=50 (weibull)","n=100 (gamma)","n=250 (weibull)","n=500 (weibull)"),c("boot(50%)","boot(75%)","boot(95%)")))
Table2a[1,]=abs(Quantile.best.mle.n25.barbar-Quantile.true)  #weibull
Table2a[2,]=abs(Quantile.best.mle.n50.barbar-Quantile.true)  #weibull
Table2a[3,]=abs(Quantile.best.mle.n100.barbar-Quantile.true) #gamma
Table2a[4,]=abs(Quantile.best.mle.n250.barbar-Quantile.true) #weibull
Table2a[5,]=abs(Quantile.best.mle.n500.barbar-Quantile.true) #weibull
```

```{r, include=FALSE}
Table2b=matrix(0,5,6,dimnames=list(c("n=25","n=50","n=100","n=250","n=500"),c("boot(50%)","boot(75%)","boot(95%)","table1(50%)","table1(75%)","table1(95%)")))
#n=25, best model: weibull 
Quantile.best.mle.n25=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n25[1], scale =MLE.weibull.boot.mean.n25[2])
Quantile.true
Quantile.best.mle.n25
Table2b[1,c(1,2,3)]=abs(Quantile.best.mle.n25-Quantile.true)
#n=50, best model: weibull
Quantile.best.mle.n50=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n50[1], scale =MLE.weibull.boot.mean.n50[2])
Quantile.true
Quantile.best.mle.n50
Table2b[2,c(1,2,3)]=abs(Quantile.best.mle.n50-Quantile.true)
#n=100, best model: gamma
Quantile.best.mle.n100=qgamma(c(0.5,0.75,0.95), shape = MLE.gamma.boot.mean.n100[1], rate = MLE.gamma.boot.mean.n100[2])
Quantile.true
Quantile.best.mle.n100
Table2b[3,c(1,2,3)]=abs(Quantile.best.mle.n100-Quantile.true)
#n=250, best model: weibull
Quantile.best.mle.n250=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n250[1], scale =MLE.weibull.boot.mean.n250[2])
Quantile.true
Quantile.best.mle.n250
Table2b[4,c(1,2,3)]=abs(Quantile.best.mle.n250-Quantile.true)
#n=500, best model: weibull
Quantile.best.mle.n500=qweibull(c(0.5,0.75,0.95), shape = MLE.weibull.boot.mean.n500[1], scale = MLE.weibull.boot.mean.n500[2])
Quantile.true
Quantile.best.mle.n500
Table2b[5,c(1,2,3)]=abs(Quantile.best.mle.n500-Quantile.true)
#From Table 1 before
Table2b[1,c(4,5,6)]=Table1.1[4,c(2,4,6)] #weibull
Table2b[2,c(4,5,6)]=Table1.2[4,c(2,4,6)] #weibull
Table2b[3,c(4,5,6)]=Table1.3[3,c(2,4,6)] #gamma
Table2b[4,c(4,5,6)]=Table1.4[4,c(2,4,6)] #weibull
Table2b[5,c(4,5,6)]=Table1.5[4,c(2,4,6)] #weibull
```

The summary of the quantile bootstrap errors -- without any outlier detection -- is given below:
  
  
```{r TableBootErrSummaryS2a, echo=T, comment = NA}
kable(Table2a, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (3) for all moderately heavy tailed Weibull samples")
```

```{r TableBootErrSummaryS2b, echo=T, comment = NA}
colnames(Table2b) <- c("q_boot(50%)","q_boot(75%)","q_boot(95%)", "q_sample(50%)","q_sample(75%)","q_sample(95%)")
rownames(Table2b) <- c("n=25 (weibull)", "n=50 (weibull)", "n=100 (gamma)", "n=250 (weibull)", "n=500 (weibull)")
kable(Table2b, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (4) and (5) for all moderately heavy tailed Weibull samples")
```

The estimation errors show a reasonable pattern, since the larger the sample is the smaller the error is. Note that outliers are not removed in this section. You should also note that the *true* quantile values are $q(50\%)=0.480453$, $q(50\%)=1.921812$ and $q(95\%)=8.974412$, so estimation errors are very small but proportional to the *true* values.
 


## Weibull Sample with Very Light Tail {#S3}

We now redo the computations from Sections \@ref(MLE), \@ref(MOM) and \@ref(S1) for **Sample 3**, which is drawn from the Weibull distribution with parameters $a=2$ and $b=1$.  Note that this particular Weibull distribution is a **very light tailed** distribution since $a>1$. We only report the very final R outputs, since the intermediate steps are very similar to those from **Sample 1** computations. 

Note that *set.seed(1234)* is the seed set for, but no seed is set for bootstrapping computations. 

```{r}
set.seed(1234) #Generating Sample1 and saved in a list
n=c(25, 50, 100, 250, 500)
weibull.shape=2
weibull.scale=1 
Sample3.list=list(n25=rep(0,25),n50=rep(0,50),n100=rep(0,100),n250=rep(0,250),n500=rep(0,500))
for (i in 1:length(n)) {
Sample3=rweibull(n[i], shape=weibull.shape , scale=weibull.scale)  
Sample3.list[[i]]=Sample3
}
```
```{r,echo=FALSE, include=FALSE}
#MLE and MME for 8 different distributions and saved in each matrix
M1.MLE.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MLE.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MLE.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MLE.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MLE.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MLE.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MLE.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MLE.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
M1.MME.lnorm=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("meanlog","sdlog")))
M2.MME.exp=matrix(0,length(n),1,dimnames=list(c("n25","n50","n100","n250","n500"),c("rate")))
M3.MME.gamma=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","rate")))
M4.MME.weibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M5.MME.llogis=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M6.MME.invweibull=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M7.MME.pareto=matrix(0,length(n),2,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape","scale")))
M8.MME.trgamma=matrix(0,length(n),3,dimnames=list(c("n25","n50","n100","n250","n500"),c("shape1","shape2","rate")))
for (i in 1:length(n)) {
#lnorm
M1.MLE.lnorm[i,]=mledist(Sample3.list[[i]], "lnorm")$estimate
M1.MME.lnorm[i,]=mmedist(Sample3.list[[i]], "lnorm")$estimate
#exp
MLE.exp=mledist(Sample3.list[[i]], "exp")
if (is.na(MLE.exp$estimate)==is.na(NA)) {MLE.exp$estimate=1/mean(Sample3.list[[i]])}
M2.MLE.exp[i,]=MLE.exp$estimate
M2.MME.exp[i,]=mmedist(Sample3.list[[i]], "exp")$estimate
#gamma
MLE.gamma=mledist(Sample3.list[[i]], "gamma")  #when the meanlog is too large, become NA
M3.MLE.gamma[i,]=MLE.gamma$estimate
if (is.na(MLE.gamma$estimate[1])==is.na(NA)) {
  MLE.gamma$estimate=gammaMLE(Sample3.list[[i]])
  MLE.gamma=c(MLE.gamma$estimate[1],1/MLE.gamma$estimate[2])
  M3.MLE.gamma[i,]=MLE.gamma}
M3.MME.gamma[i,]=mmedist(Sample3.list[[i]], "gamma")$estimate
#weibull
M4.MLE.weibull[i,]=mledist(Sample3.list[[i]], "weibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.weibull=mmedist(Sample3.list[[i]], "weibull", order=c(1, 2), memp=memp)
M4.MME.weibull[i,]=MME.weibull$estimate
#llogis
M5.MLE.llogis[i,]=mledist(Sample3.list[[i]], "llogis")$estimate
memp  <-  function(x, order) mean(x^order)
MME.llogis=mmedist(Sample3.list[[i]], "llogis", order=c(1, 2), memp=memp)
M5.MME.llogis[i,]=MME.llogis$estimate
#invweibull  
M6.MLE.invweibull[i,]=MLE.invweibull=mledist(Sample3.list[[i]], "invweibull")$estimate
memp  <-  function(x, order) mean(x^order)
MME.invweibull=mmedist(Sample3.list[[i]], "invweibull", order=c(1, 2), memp=memp)
M6.MME.invweibull[i,]=MME.invweibull$estimate
#pareto
M7.MLE.pareto[i,]=MLE.pareto=mledist(Sample3.list[[i]], "pareto")$estimate
memp  <-  function(x, order) mean(x^order)
MME.pareto=mmedist(Sample3.list[[i]], "pareto", order=c(1, 2), memp=memp)
M7.MME.pareto[i,]=MME.pareto$estimate
#trgamma
M8.MLE.trgamma[i,]=mledist(Sample3.list[[i]], "trgamma")$estimate
memp  <-  function(x, order) mean(x^order)
MME.trgamma=mmedist(Sample3.list[[i]], "trgamma", order=c(1, 2, 3), memp=memp)
M8.MME.trgamma[i,]=MME.trgamma$estimate
}
```

```{r, echo=FALSE, include=FALSE}
#Quantile of the true distribution
Quantile.true=qweibull(c(0.5,0.75,0.95), shape=weibull.shape, scale=weibull.scale, lower.tail = TRUE, log.p = FALSE)
Quantile.true=matrix(Quantile.true,1,3,dimnames=list(c("quantile (true dist)"),c("(50%)","(75%)","(95%)")))
#Quantile.true
```

```{r, echo=FALSE, include=FALSE}
#Table 1.1 (n=25)
Table1.1=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n25 <- fitdist(Sample3.list[[1]], "lnorm", method="mle")
Quantile.lnorm.mle.n25=quantile(mle.fln.n25, probs = c(0.5, 0.75, 0.95))
mme.fln.n25 <- fitdist(Sample3.list[[1]], "lnorm", method="mme")
Quantile.lnorm.mme.n25=quantile(mme.fln.n25, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n25 <- fitdist(Sample3.list[[1]], "exp", method="mle")
Quantile.exp.mle.n25=quantile(mle.fe.n25, probs = c(0.5, 0.75, 0.95))
mme.fe.n25 <- fitdist(Sample3.list[[1]], "exp", method="mme")
Quantile.exp.mme.n25=quantile(mme.fe.n25, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n25 <- fitdist(Sample3.list[[1]], "gamma", method="mle")
Quantile.gamma.mle.n25=quantile(mle.fg.n25, probs = c(0.5, 0.75, 0.95))
mme.fg.n25 <- fitdist(Sample3.list[[1]], "gamma", method="mme")
Quantile.gamma.mme.n25=quantile(mme.fg.n25, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n25 <- fitdist(Sample3.list[[1]], "weibull", method="mle")
Quantile.weibull.mle.n25=quantile(mle.fw.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n25 <- fitdist(Sample3.list[[1]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n25=quantile(mme.fw.n25, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n25 <- fitdist(Sample3.list[[1]], "llogis", method="mle")
Quantile.llogis.mle.n25=quantile(mle.fl.n25, probs = c(0.5, 0.75, 0.95))
mme.fl.n25 <- fitdist(Sample3.list[[1]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n25=quantile(mme.fl.n25, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n25 <- fitdist(Sample3.list[[1]], "invweibull", method="mle")
Quantile.invweibull.mle.n25=quantile(mle.fi.n25, probs = c(0.5, 0.75, 0.95))
mme.fi.n25 <- fitdist(Sample3.list[[1]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n25=quantile(mme.fi.n25, probs = c(0.5, 0.75, 0.95))
#pareto
#mle.fp.n25 <- fitdist(Sample3.list[[1]], "pareto", method="mle")
#Quantile.pareto.mle.n25=quantile(mle.fp.n25, probs = c(0.5, 0.75, 0.95))
mme.fp.n25 <- fitdist(Sample3.list[[1]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n25=quantile(mme.fp.n25, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n25 <- fitdist(Sample3.list[[1]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n25=quantile(mle.ft.n25, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n25 <- fitdist(Sample3.list[[1]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n25=quantile(mme.ft.n25, probs = c(0.5, 0.75, 0.95))
#Table
Table1.1[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n25$quantiles)-Quantile.true)
Table1.1[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n25$quantiles)-Quantile.true)
Table1.1[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n25$quantiles)-Quantile.true)
Table1.1[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n25$quantiles)-Quantile.true)
Table1.1[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n25$quantiles)-Quantile.true)
Table1.1[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n25$quantiles)-Quantile.true)
Table1.1[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n25$quantiles)-Quantile.true)
Table1.1[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n25$quantiles)-Quantile.true)
Table1.1[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n25$quantiles)-Quantile.true)
Table1.1[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n25$quantiles)-Quantile.true)
Table1.1[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n25$quantiles)-Quantile.true)
Table1.1[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n25$quantiles)-Quantile.true)
#Table1.1[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n25$quantiles)-Quantile.true)
Table1.1[7,c(2,4,6)]=c(NA,NA,NA)
Table1.1[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n25$quantiles)-Quantile.true)
Table1.1[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n25$quantiles)-Quantile.true)
Table1.1[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n25$quantiles)-Quantile.true)
#Table1.1
```


```{r, echo=FALSE, include=FALSE}
#Table 1.2 (n=50)
Table1.2=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n50 <- fitdist(Sample3.list[[2]], "lnorm", method="mle")
Quantile.lnorm.mle.n50=quantile(mle.fln.n50, probs = c(0.5, 0.75, 0.95))
mme.fln.n50 <- fitdist(Sample3.list[[2]], "lnorm", method="mme")
Quantile.lnorm.mme.n50=quantile(mme.fln.n50, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n50 <- fitdist(Sample3.list[[2]], "exp", method="mle")
Quantile.exp.mle.n50=quantile(mle.fe.n50, probs = c(0.5, 0.75, 0.95))
mme.fe.n50 <- fitdist(Sample3.list[[2]], "exp", method="mme")
Quantile.exp.mme.n50=quantile(mme.fe.n50, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n50 <- fitdist(Sample3.list[[2]], "gamma", method="mle")
Quantile.gamma.mle.n50=quantile(mle.fg.n50, probs = c(0.5, 0.75, 0.95))
mme.fg.n50 <- fitdist(Sample3.list[[2]], "gamma", method="mme")
Quantile.gamma.mme.n50=quantile(mme.fg.n50, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n50 <- fitdist(Sample3.list[[2]], "weibull", method="mle")
Quantile.weibull.mle.n50=quantile(mle.fw.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n50 <- fitdist(Sample3.list[[2]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n50=quantile(mme.fw.n50, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n50 <- fitdist(Sample3.list[[2]], "llogis", method="mle")
Quantile.llogis.mle.n50=quantile(mle.fl.n50, probs = c(0.5, 0.75, 0.95))
mme.fl.n50 <- fitdist(Sample3.list[[2]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n50=quantile(mme.fl.n50, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n50 <- fitdist(Sample3.list[[2]], "invweibull", method="mle")
Quantile.invweibull.mle.n50=quantile(mle.fi.n50, probs = c(0.5, 0.75, 0.95))
mme.fi.n50 <- fitdist(Sample3.list[[2]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n50=quantile(mme.fi.n50, probs = c(0.5, 0.75, 0.95))
#pareto
#mle.fp.n50 <- fitdist(Sample3.list[[2]], "pareto", method="mle")
#Quantile.pareto.mle.n50=quantile(mle.fp.n50, probs = c(0.5, 0.75, 0.95))
mme.fp.n50 <- fitdist(Sample3.list[[2]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n50=quantile(mme.fp.n50, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n50 <- fitdist(Sample3.list[[2]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n50=quantile(mle.ft.n50, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n50 <- fitdist(Sample3.list[[2]], "trgamma", method="mme",optim.method="SANN", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n50=quantile(mme.ft.n50, probs = c(0.5, 0.75, 0.95))
#Table
Table1.2[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n50$quantiles)-Quantile.true)
Table1.2[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n50$quantiles)-Quantile.true)
Table1.2[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n50$quantiles)-Quantile.true)
Table1.2[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n50$quantiles)-Quantile.true)
Table1.2[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n50$quantiles)-Quantile.true)
Table1.2[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n50$quantiles)-Quantile.true)
Table1.2[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n50$quantiles)-Quantile.true)
Table1.2[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n50$quantiles)-Quantile.true)
Table1.2[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n50$quantiles)-Quantile.true)
Table1.2[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n50$quantiles)-Quantile.true)
Table1.2[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n50$quantiles)-Quantile.true)
Table1.2[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n50$quantiles)-Quantile.true)
#Table1.2[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n50$quantiles)-Quantile.true)
Table1.2[7,c(2,4,6)]=c(NA,NA,NA)
Table1.2[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n50$quantiles)-Quantile.true)
Table1.2[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n50$quantiles)-Quantile.true)
Table1.2[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n50$quantiles)-Quantile.true)
#Table1.2
```


```{r, echo=FALSE, include=FALSE}
#Table 1.3 (n=100)
Table1.3=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","weibull","llogis","invwei","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n100 <- fitdist(Sample3.list[[3]], "lnorm", method="mle")
Quantile.lnorm.mle.n100=quantile(mle.fln.n100, probs = c(0.5, 0.75, 0.95))
mme.fln.n100 <- fitdist(Sample3.list[[3]], "lnorm", method="mme")
Quantile.lnorm.mme.n100=quantile(mme.fln.n100, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n100 <- fitdist(Sample3.list[[3]], "exp", method="mle")
Quantile.exp.mle.n100=quantile(mle.fe.n100, probs = c(0.5, 0.75, 0.95))
mme.fe.n100 <- fitdist(Sample3.list[[3]], "exp", method="mme")
Quantile.exp.mme.n100=quantile(mme.fe.n100, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n100 <- fitdist(Sample3.list[[3]], "gamma", method="mle")
Quantile.gamma.mle.n100=quantile(mle.fg.n100, probs = c(0.5, 0.75, 0.95))
mme.fg.n100 <- fitdist(Sample3.list[[3]], "gamma", method="mme")
Quantile.gamma.mme.n100=quantile(mme.fg.n100, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n100 <- fitdist(Sample3.list[[3]], "weibull", method="mle")
Quantile.weibull.mle.n100=quantile(mle.fw.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n100 <- fitdist(Sample3.list[[3]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n100=quantile(mme.fw.n100, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n100 <- fitdist(Sample3.list[[3]], "llogis", method="mle")
Quantile.llogis.mle.n100=quantile(mle.fl.n100, probs = c(0.5, 0.75, 0.95))
mme.fl.n100 <- fitdist(Sample3.list[[3]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n100=quantile(mme.fl.n100, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n100 <- fitdist(Sample3.list[[3]], "invweibull", method="mle")
Quantile.invweibull.mle.n100=quantile(mle.fi.n100, probs = c(0.5, 0.75, 0.95))
mme.fi.n100 <- fitdist(Sample3.list[[3]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n100=quantile(mme.fi.n100, probs = c(0.5, 0.75, 0.95))
#pareto
#mle.fp.n100 <- fitdist(Sample3.list[[3]], "pareto", method="mle")
#Quantile.pareto.mle.n100=quantile(mle.fp.n100, probs = c(0.5, 0.75, 0.95))
mme.fp.n100 <- fitdist(Sample3.list[[3]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n100=quantile(mme.fp.n100, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n100 <- fitdist(Sample3.list[[3]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n100=quantile(mle.ft.n100, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n100 <- fitdist(Sample3.list[[3]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n100=quantile(mme.ft.n100, probs = c(0.5, 0.75, 0.95))
#Table
Table1.3[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n100$quantiles)-Quantile.true)
Table1.3[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n100$quantiles)-Quantile.true)
Table1.3[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n100$quantiles)-Quantile.true)
Table1.3[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n100$quantiles)-Quantile.true)
Table1.3[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n100$quantiles)-Quantile.true)
Table1.3[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n100$quantiles)-Quantile.true)
Table1.3[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n100$quantiles)-Quantile.true)
Table1.3[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n100$quantiles)-Quantile.true)
Table1.3[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n100$quantiles)-Quantile.true)
Table1.3[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n100$quantiles)-Quantile.true)
Table1.3[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n100$quantiles)-Quantile.true)
Table1.3[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n100$quantiles)-Quantile.true)
#Table1.3[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n100$quantiles)-Quantile.true)
Table1.3[7,c(2,4,6)]=c(NA,NA,NA)
Table1.3[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n100$quantiles)-Quantile.true)
Table1.3[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n100$quantiles)-Quantile.true)
Table1.3[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n100$quantiles)-Quantile.true)
#Table1.3
```


```{r, echo=FALSE, include=FALSE}
#Table 1.4 (n=250)
Table1.4=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n250 <- fitdist(Sample3.list[[4]], "lnorm", method="mle")
Quantile.lnorm.mle.n250=quantile(mle.fln.n250, probs = c(0.5, 0.75, 0.95))
mme.fln.n250 <- fitdist(Sample3.list[[4]], "lnorm", method="mme")
Quantile.lnorm.mme.n250=quantile(mme.fln.n250, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n250 <- fitdist(Sample3.list[[4]], "exp", method="mle")
Quantile.exp.mle.n250=quantile(mle.fe.n250, probs = c(0.5, 0.75, 0.95))
mme.fe.n250 <- fitdist(Sample3.list[[4]], "exp", method="mme")
Quantile.exp.mme.n250=quantile(mme.fe.n250, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n250 <- fitdist(Sample3.list[[4]], "gamma", method="mle")
Quantile.gamma.mle.n250=quantile(mle.fg.n250, probs = c(0.5, 0.75, 0.95))
mme.fg.n250 <- fitdist(Sample3.list[[4]], "gamma", method="mme")
Quantile.gamma.mme.n250=quantile(mme.fg.n250, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n250 <- fitdist(Sample3.list[[4]], "weibull", method="mle")
Quantile.weibull.mle.n250=quantile(mle.fw.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n250 <- fitdist(Sample3.list[[4]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n250=quantile(mme.fw.n250, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n250 <- fitdist(Sample3.list[[4]], "llogis", method="mle")
Quantile.llogis.mle.n250=quantile(mle.fl.n250, probs = c(0.5, 0.75, 0.95))
mme.fl.n250 <- fitdist(Sample3.list[[4]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n250=quantile(mme.fl.n250, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n250 <- fitdist(Sample3.list[[4]], "invweibull", method="mle")
Quantile.invweibull.mle.n250=quantile(mle.fi.n250, probs = c(0.5, 0.75, 0.95))
mme.fi.n250 <- fitdist(Sample3.list[[4]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n250=quantile(mme.fi.n250, probs = c(0.5, 0.75, 0.95))
#pareto
#mle.fp.n250 <- fitdist(Sample3.list[[4]], "pareto", method="mle")
#Quantile.pareto.mle.n250=quantile(mle.fp.n250, probs = c(0.5, 0.75, 0.95))
mme.fp.n250 <- fitdist(Sample3.list[[4]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n250=quantile(mme.fp.n250, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n250 <- fitdist(Sample3.list[[4]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n250=quantile(mle.ft.n250, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n250 <- fitdist(Sample3.list[[4]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n250=quantile(mme.ft.n250, probs = c(0.5, 0.75, 0.95))
#Table
Table1.4[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n250$quantiles)-Quantile.true)
Table1.4[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n250$quantiles)-Quantile.true)
Table1.4[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n250$quantiles)-Quantile.true)
Table1.4[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n250$quantiles)-Quantile.true)
Table1.4[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n250$quantiles)-Quantile.true)
Table1.4[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n250$quantiles)-Quantile.true)
Table1.4[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n250$quantiles)-Quantile.true)
Table1.4[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n250$quantiles)-Quantile.true)
Table1.4[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n250$quantiles)-Quantile.true)
Table1.4[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n250$quantiles)-Quantile.true)
Table1.4[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n250$quantiles)-Quantile.true)
Table1.4[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n250$quantiles)-Quantile.true)
#Table1.4[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n250$quantiles)-Quantile.true)
Table1.4[7,c(2,4,6)]=c(NA,NA,NA)
Table1.4[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n250$quantiles)-Quantile.true)
Table1.4[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n250$quantiles)-Quantile.true)
Table1.4[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n250$quantiles)-Quantile.true)
#Table1.4
```

```{r, echo=FALSE, include=FALSE}
#Table 1.5 (n=500)
Table1.5=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.n500 <- fitdist(Sample3.list[[5]], "lnorm", method="mle")
Quantile.lnorm.mle.n500=quantile(mle.fln.n500, probs = c(0.5, 0.75, 0.95))
mme.fln.n500 <- fitdist(Sample3.list[[5]], "lnorm", method="mme")
Quantile.lnorm.mme.n500=quantile(mme.fln.n500, probs = c(0.5, 0.75, 0.95))
#exp
mle.fe.n500 <- fitdist(Sample3.list[[5]], "exp", method="mle")
Quantile.exp.mle.n500=quantile(mle.fe.n500, probs = c(0.5, 0.75, 0.95))
mme.fe.n500 <- fitdist(Sample3.list[[5]], "exp", method="mme")
Quantile.exp.mme.n500=quantile(mme.fe.n500, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.n500 <- fitdist(Sample3.list[[5]], "gamma", method="mle")
Quantile.gamma.mle.n500=quantile(mle.fg.n500, probs = c(0.5, 0.75, 0.95))
mme.fg.n500 <- fitdist(Sample3.list[[5]], "gamma", method="mme")
Quantile.gamma.mme.n500=quantile(mme.fg.n500, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.n500 <- fitdist(Sample3.list[[5]], "weibull", method="mle")
Quantile.weibull.mle.n500=quantile(mle.fw.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.n500 <- fitdist(Sample3.list[[5]], "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.n500=quantile(mme.fw.n500, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.n500 <- fitdist(Sample3.list[[5]], "llogis", method="mle")
Quantile.llogis.mle.n500=quantile(mle.fl.n500, probs = c(0.5, 0.75, 0.95))
mme.fl.n500 <- fitdist(Sample3.list[[5]], "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.n500=quantile(mme.fl.n500, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.n500 <- fitdist(Sample3.list[[5]], "invweibull", method="mle")
Quantile.invweibull.mle.n500=quantile(mle.fi.n500, probs = c(0.5, 0.75, 0.95))
mme.fi.n500 <- fitdist(Sample3.list[[5]], "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.n500=quantile(mme.fi.n500, probs = c(0.5, 0.75, 0.95))
#pareto
#mle.fp.n500 <- fitdist(Sample3.list[[5]], "pareto", method="mle")
#Quantile.pareto.mle.n500=quantile(mle.fp.n500, probs = c(0.5, 0.75, 0.95))
mme.fp.n500 <- fitdist(Sample3.list[[5]], "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.n500=quantile(mme.fp.n500, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.n500 <- fitdist(Sample3.list[[5]], "trgamma", method="mle",optim.method="SANN")
Quantile.trgamma.mle.n500=quantile(mle.ft.n500, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.n500 <- fitdist(Sample3.list[[5]], "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.n500=quantile(mme.ft.n500, probs = c(0.5, 0.75, 0.95))
#Table
Table1.5[1,c(2,4,6)]=abs(as.numeric(Quantile.lnorm.mle.n500$quantiles)-Quantile.true)
Table1.5[1,c(1,3,5)]=abs(as.numeric(Quantile.lnorm.mme.n500$quantiles)-Quantile.true)
Table1.5[2,c(2,4,6)]=abs(as.numeric(Quantile.exp.mle.n500$quantiles)-Quantile.true)
Table1.5[2,c(1,3,5)]=abs(as.numeric(Quantile.exp.mme.n500$quantiles)-Quantile.true)
Table1.5[3,c(2,4,6)]=abs(as.numeric(Quantile.gamma.mle.n500$quantiles)-Quantile.true)
Table1.5[3,c(1,3,5)]=abs(as.numeric(Quantile.gamma.mme.n500$quantiles)-Quantile.true)
Table1.5[4,c(2,4,6)]=abs(as.numeric(Quantile.weibull.mle.n500$quantiles)-Quantile.true)
Table1.5[4,c(1,3,5)]=abs(as.numeric(Quantile.weibull.mme.n500$quantiles)-Quantile.true)
Table1.5[5,c(2,4,6)]=abs(as.numeric(Quantile.llogis.mle.n500$quantiles)-Quantile.true)
Table1.5[5,c(1,3,5)]=abs(as.numeric(Quantile.llogis.mme.n500$quantiles)-Quantile.true)
Table1.5[6,c(2,4,6)]=abs(as.numeric(Quantile.invweibull.mle.n500$quantiles)-Quantile.true)
Table1.5[6,c(1,3,5)]=abs(as.numeric(Quantile.invweibull.mme.n500$quantiles)-Quantile.true)
#Table1.5[7,c(2,4,6)]=abs(as.numeric(Quantile.pareto.mle.n500$quantiles)-Quantile.true)
Table1.5[7,c(2,4,6)]=c(NA,NA,NA)
Table1.5[7,c(1,3,5)]=abs(as.numeric(Quantile.pareto.mme.n500$quantiles)-Quantile.true)
Table1.5[8,c(2,4,6)]=abs(as.numeric(Quantile.trgamma.mle.n500$quantiles)-Quantile.true)
Table1.5[8,c(1,3,5)]=abs(as.numeric(Quantile.trgamma.mme.n500$quantiles)-Quantile.true)
#Table1.5
```

```{r, echo=FALSE, include=FALSE}
Table.AIC=matrix(0,8,5,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC[1,]=c(mle.fln.n25$aic, mle.fln.n50$aic, mle.fln.n100$aic, mle.fln.n250$aic, mle.fln.n500$aic)
Table.AIC[2,]=c(mle.fe.n25$aic, mle.fe.n50$aic, mle.fe.n100$aic, mle.fe.n250$aic, mle.fe.n500$aic)
Table.AIC[3,]=c(mle.fg.n25$aic, mle.fg.n50$aic, mle.fg.n100$aic, mle.fg.n250$aic, mle.fg.n500$aic)
Table.AIC[4,]=c(mle.fw.n25$aic, mle.fw.n50$aic, mle.fw.n100$aic, mle.fw.n250$aic, mle.fw.n500$aic)
Table.AIC[5,]=c(mle.fl.n25$aic, mle.fl.n50$aic, mle.fl.n100$aic, mle.fl.n250$aic, mle.fl.n500$aic)
Table.AIC[6,]=c(mle.fi.n25$aic, mle.fi.n50$aic, mle.fi.n100$aic, mle.fi.n250$aic, mle.fi.n500$aic)
Table.AIC[7,]=c(mle.fp.n25$aic, mle.fp.n50$aic, mle.fp.n100$aic, mle.fp.n250$aic, mle.fp.n500$aic)
Table.AIC[8,]=c(mle.ft.n25$aic, mle.ft.n50$aic, mle.ft.n100$aic, mle.ft.n250$aic, mle.ft.n500$aic)
Table.AIC.min=matrix(0,1,5,dimnames=list(c("Min  (AIC)"),c("(MLE,n=25)","(MLE,n=50)","(MLE,n=100)","(MLE,n=250)","(MLE,n=500)")))
Table.AIC.min[1,]=c(rownames(Table.AIC)[which(Table.AIC[,1] == min(Table.AIC[,1]))],
                   rownames(Table.AIC)[which(Table.AIC[,2] == min(Table.AIC[,2]))],
                   rownames(Table.AIC)[which(Table.AIC[,3] == min(Table.AIC[,3]))],
                   rownames(Table.AIC)[which(Table.AIC[,4] == min(Table.AIC[,4]))],
                   rownames(Table.AIC)[which(Table.AIC[,5] == min(Table.AIC[,5]))])
```

```{r AICS3, echo=T, comment = NA}
kable(Table.AIC, digits = 3, format.args = list(big.mark = ","), caption=("AIC summary for very light tailed Weibull samples of size n=25, n=50, n=100, n=250 and n=500"))
```

```{r AICS3min, echo=T, comment = NA}
kable(Table.AIC.min, digits = 3, format.args = list(big.mark = ","), caption=("Best (or DISToo) distribution chosen through AIC for very light tailed Weibull samples of size n=25, n=50, n=100, n=250 and n=500"))
```
It looks like AIC does make the right choice, except of the Weibull samples of size $n=25$ and $n=100$, though *gamma* and *trgamma* marginally surpass the `true' choice, namely *weibull*. Since MLE estimation for *trgamma* is not very stable -- there are three parameters to estimate -- we will further assume that the `best' AIC choice for the Weibull sample of size $n=100$ is *weibull*, which is the second `best' AIC-based choice though is not far from the `best` choice.

We now perform the usual bootstrap sampling.

```{r, include=FALSE}
#Sample3: the case of n=25, \textbf{gamma} based on min(AIC), samples with replacement=25
MLE.gamma.boot.n25=matrix(0,25,2,dimnames=list(c(1:25),c("shape","rate")))
for (b in 1:25) {
Sample3.boot=sample(Sample3.list[[1]], replace = TRUE)
MLE.gamma=mledist(Sample3.boot, "gamma")
MLE.gamma.boot.n25[b,]=MLE.gamma$estimate
}
MLE.gamma.boot.mean.n25=colMeans(MLE.gamma.boot.n25)
```
```{r, include=FALSE}
#Sample3: the case of n=50, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n50=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample3.boot=sample(Sample3.list[[2]], replace = TRUE)
MLE.weibull=mledist(Sample3.boot, "weibull")
MLE.weibull.boot.n50[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n50=colMeans(MLE.weibull.boot.n50)
```
```{r, include=FALSE}
#Sample3: the case of n=100, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n100=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample3.boot=sample(Sample3.list[[3]], replace = TRUE)
MLE.weibull=mledist(Sample3.boot, "weibull")
MLE.weibull.boot.n100[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n100=colMeans(MLE.weibull.boot.n100)
```
```{r, include=FALSE}
#Sample3: the case of n=250, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n250=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample3.boot=sample(Sample3.list[[4]], replace = TRUE)
MLE.weibull=mledist(Sample3.boot, "weibull")
MLE.weibull.boot.n250[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n250=colMeans(MLE.weibull.boot.n250)
```
```{r, include=FALSE}
#Sample3: the case of n=500, \textbf{weibull} based on min(AIC), samples with replacement=25
MLE.weibull.boot.n500=matrix(0,25,2,dimnames=list(c(1:25),c("shape","scale")))
for (b in 1:25) {
Sample3.boot=sample(Sample3.list[[5]], replace = TRUE)
MLE.weibull=mledist(Sample3.boot, "weibull")
MLE.weibull.boot.n500[b,]=MLE.weibull$estimate
}
MLE.weibull.boot.mean.n500=colMeans(MLE.weibull.boot.n500)
```


The sets of $n_b=25$ MLE estimates are now averaged and obtain the so-called *Bootstrap MLE estimates*, which are tabulated below. 


```{r BootEstW25S3, echo=T, comment = NA}
kable(MLE.gamma.boot.mean.n25, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming gamma (n=25)")
```

```{r BootEstW50S3, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n50, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=50)")
```


```{r BootEstW100S3, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n100, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=250)")
```

```{r BootEstW250S3, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n250, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=250)")
```

```{r BootEstW500S3, echo=T, comment = NA}
kable(MLE.weibull.boot.mean.n500, digits = 3, format.args = list(big.mark = ","), caption="Bootstrap MLE estimates assuming weibull (n=500)")
```

As before, we report three sets of estimation errors as follows:

\begin{eqnarray*} 
&&(3)\;\text{Report} \;\big|\bar{\bar{q}}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(4)\;\text{Report} \;\big|\bar{q}^{MLE}_{boot} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\},\\
&&(5)\;\text{Report} \;\big|\bar{q}^{MLE}_{sample} (xx;n)-xx\%\mbox{quantiles of the 'true' distribution}\big|\quad \text{for all}\;n=\{25, 50, 100, 250,  500\}.
\end{eqnarray*}

```{r, include=FALSE}
#(best n=25) gamma
Quantile.best.mle.n25.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n25.each[i,]=qgamma(c(0.5,0.75,0.95), shape=MLE.gamma.boot.n25[i,1], rate=MLE.gamma.boot.n25[i,2])
}
Quantile.best.mle.n25.barbar=colMeans(Quantile.best.mle.n25.each)
#(best n=50) weibull
Quantile.best.mle.n50.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n50.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n50[i,1], scale=MLE.weibull.boot.n50[i,2])
}
Quantile.best.mle.n50.barbar=colMeans(Quantile.best.mle.n50.each)
#(best n=100) weibull -- instead of trgamma --
Quantile.best.mle.n25.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n100.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n100[i,1], scale=MLE.weibull.boot.n100[i,2])
}
Quantile.best.mle.n100.barbar=colMeans(Quantile.best.mle.n100.each)
#(best n=250) weibull
Quantile.best.mle.n250.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n250.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n250[i,1], scale=MLE.weibull.boot.n250[i,2])
}
Quantile.best.mle.n250.barbar=colMeans(Quantile.best.mle.n250.each)
#(best n=500) weibull
Quantile.best.mle.n500.each=matrix(0,25,3,dimnames=list(NULL,c("q_boot(50%)","q_boot(75%)","q_boot(95%)")))
for (i in 1:25) {
Quantile.best.mle.n500.each[i,]=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.n500[i,1], scale=MLE.weibull.boot.n500[i,2])
}
Quantile.best.mle.n500.barbar=colMeans(Quantile.best.mle.n500.each)
# Table2a
Table2a=matrix(0,5,3,dimnames=list(c("n=25 (gamma)","n=50 (weibull)","n=100 (weibull)","n=250 (weibull)","n=500 (weibull)"),c("boot(50%)","boot(75%)","boot(95%)")))
Table2a[1,]=abs(Quantile.best.mle.n25.barbar-Quantile.true)  #gamma
Table2a[2,]=abs(Quantile.best.mle.n50.barbar-Quantile.true)  #weibull
Table2a[3,]=abs(Quantile.best.mle.n100.barbar-Quantile.true) #weibull
Table2a[4,]=abs(Quantile.best.mle.n250.barbar-Quantile.true) #weibull
Table2a[5,]=abs(Quantile.best.mle.n500.barbar-Quantile.true) #weibull
```
```{r, include=FALSE}
Table2=matrix(0,5,6,dimnames=list(c("n=25","n=50","n=100","n=250","n=500"),c("boot(50%)","boot(75%)","boot(95%)","table1(50%)","table1(75%)","table1(95%)")))
#n=25, best model: gamma
Quantile.best.mle.n25=qgamma(c(0.5,0.75,0.95), shape=MLE.gamma.boot.mean.n25[1], rate =MLE.gamma.boot.mean.n25[2])
Quantile.true
Quantile.best.mle.n25
Table2[1,c(1,2,3)]=abs(Quantile.best.mle.n25-Quantile.true)
#n=50, best model: weibull
Quantile.best.mle.n50=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n50[1], scale =MLE.weibull.boot.mean.n50[2])
Quantile.true
Quantile.best.mle.n50
Table2[2,c(1,2,3)]=abs(Quantile.best.mle.n50-Quantile.true)
#n=100, second best model: weibull -- instead of trgamma --
Quantile.best.mle.n100=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n100[1], scale =MLE.weibull.boot.mean.n100[2])
Quantile.true
Quantile.best.mle.n100
Table2[3,c(1,2,3)]=abs(Quantile.best.mle.n100-Quantile.true)
#n=250, best model: weibull
Quantile.best.mle.n250=qweibull(c(0.5,0.75,0.95), shape=MLE.weibull.boot.mean.n250[1], scale =MLE.weibull.boot.mean.n250[2])
Quantile.true
Quantile.best.mle.n250
Table2[4,c(1,2,3)]=abs(Quantile.best.mle.n250-Quantile.true)
#n=500, best model: weibull
Quantile.best.mle.n500=qweibull(c(0.5,0.75,0.95), shape = MLE.weibull.boot.mean.n500[1], scale = MLE.weibull.boot.mean.n500[2])
Quantile.true
Quantile.best.mle.n500
Table2[5,c(1,2,3)]=abs(Quantile.best.mle.n500-Quantile.true)
#From Table 1 before
Table2[1,c(4,5,6)]=Table1.1[3,c(2,4,6)] #gamma
Table2[2,c(4,5,6)]=Table1.2[4,c(2,4,6)] #weibull
Table2[3,c(4,5,6)]=Table1.3[4,c(2,4,6)] #weibull
Table2[4,c(4,5,6)]=Table1.4[4,c(2,4,6)] #weibull
Table2[5,c(4,5,6)]=Table1.5[4,c(2,4,6)] #weibull
```

The summary of the quantile bootstrap errors -- without any outlier detection -- is given below:
  
  
```{r TableBootErrSummaryS3a, echo=T, comment = NA}
kable(Table2a, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (3) for all very light tailed tailed Weibull samples")
```
```{r TableBootErrSummaryS3, echo=T, comment = NA}
colnames(Table2) <- c("q_boot(50%)","q_boot(75%)","q_boot(95%)", "q_sample(50%)","q_sample(75%)","q_sample(95%)")
rownames(Table2) <- c("n=25 (gamma)", "n=50 (weibull)", "n=100 (weibull)", "n=250 (weibull)", "n=500 (weibull)")
kable(Table2, digits = 3, format.args = list(big.mark = ","), caption="Quantile bootstrap estimation errors based on (4) and (5) for all very light tailed tailed Weibull samples")
```

The estimation errors show a reasonable pattern, since the larger the sample is the smaller the error is. Note that outliers are not removed in this section. You should also 
note that the *true* quantile values are $q(50\%)=0.8325546$, $q(50\%)=1.1774100$ and $q(95\%)=1.7308184$, so estimation errors are very small but proportional to the *true* values.


# Real Data Analysis {#RDA}

***

In this section, you learn to:

- Compare competitive fitting models when working with real data;
- Compute point estimates and use bootstrapping to robustify your point estimates. 

***

The real data analysis from this section is the dataset that has been described in Section \@ref(DD), and a detailed exploratory data analysis is provided in Section \@ref(EDA). We are particularly focus on **Building Insurance Claim Size** corresponding to claims recorded in 1990. 

We first fit the `best' MLE distribution from the eight possible choices, and report 

$$
\hat{q}^{MLE}_{50\%},\quad\hat{q}^{MLE}_{75\%}\quad\text{and}\quad \hat{q}^{MLE}_{95\%},
$$

i.e. the theoretical quantiles for the \textbf{best} distribution (these are three point estimates).

Note that the claims are quite large, and therefore, we scale down all claims by a factor of $100,000$, and the final outputs will be later scaled up by the same factor. 
```{r SummaryData, echo=T, comment = NA} 
#View(Bivariate_Fire_Insurance_Clean_Data)
Sample.RealData.raw=Bivariate_Fire_Insurance_Clean_Data[[2]][1336:1502]
#head(Bivariate_Fire_Insurance_Clean_Data)
#head(Sample.RealData.raw)
#tail(Sample.RealData.raw)
#In order to use exp and trgamma fitdist(), we scale the data by 100,000
Sample.RealData=Sample.RealData.raw/100000
summary(Sample.RealData)
```

```{r, echo=TRUE, include=FALSE}
Table.RealData=matrix(0,8,6,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("MOM(50%)","MLE(50%)","MOM(75%)","MLE(75%)","MOM(95%)","MLE(95%)")))
#lnorm
mle.fln.RealData <- fitdist(Sample.RealData, "lnorm", method="mle")
Quantile.lnorm.mle.RealData=quantile(mle.fln.RealData, probs = c(0.5, 0.75, 0.95))
mme.fln.RealData <- fitdist(Sample.RealData, "lnorm", method="mme")
Quantile.lnorm.mme.RealData=quantile(mme.fln.RealData, probs = c(0.5, 0.75, 0.95))
#exp
#Scale the numbers by 100,000 in order to avoid being `inf' in the exp() 
mle.fe.RealData <- fitdist(Sample.RealData, "exp", method="mle")
Quantile.exp.mle.RealData=quantile(mle.fe.RealData, probs = c(0.5, 0.75, 0.95))
mme.fe.RealData <- fitdist(Sample.RealData, "exp", method="mme")
Quantile.exp.mme.RealData=quantile(mme.fe.RealData, probs = c(0.5, 0.75, 0.95))
#gamma
mle.fg.RealData <- fitdist(Sample.RealData, "gamma", method="mle",lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.gamma.mle.RealData=quantile(mle.fg.RealData, probs = c(0.5, 0.75, 0.95))
mme.fg.RealData <- fitdist(Sample.RealData, "gamma", method="mme")
Quantile.gamma.mme.RealData=quantile(mme.fg.RealData, probs = c(0.5, 0.75, 0.95))
#weibull
mle.fw.RealData <- fitdist(Sample.RealData, "weibull", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.weibull.mle.RealData=quantile(mle.fw.RealData, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.fw.RealData <- fitdist(Sample.RealData, "weibull", method="mme", order=c(1, 2), memp=memp)
Quantile.weibull.mme.RealData=quantile(mme.fw.RealData, probs = c(0.5, 0.75, 0.95))
#llogis
mle.fl.RealData <- fitdist(Sample.RealData, "llogis", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.llogis.mle.RealData=quantile(mle.fl.RealData, probs = c(0.5, 0.75, 0.95))
mme.fl.RealData <- fitdist(Sample.RealData, "llogis", method="mme", optim.method="Nelder-Mead", order=c(1, 2), memp=memp)
Quantile.llogis.mme.RealData=quantile(mme.fl.RealData, probs = c(0.5, 0.75, 0.95))
#invweibull
mle.fi.RealData <- fitdist(Sample.RealData, "invweibull", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.invweibull.mle.RealData=quantile(mle.fi.RealData, probs = c(0.5, 0.75, 0.95))
mme.fi.RealData <- fitdist(Sample.RealData, "invweibull", method="mme", order=c(1, 2), memp=memp)
Quantile.invweibull.mme.RealData=quantile(mme.fi.RealData, probs = c(0.5, 0.75, 0.95))
#pareto
mle.fp.RealData <- fitdist(Sample.RealData, "pareto", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.pareto.mle.RealData=quantile(mle.fp.RealData, probs = c(0.5, 0.75, 0.95))
mme.fp.RealData <- fitdist(Sample.RealData, "pareto", method="mme", order=c(1, 2), memp=memp)
Quantile.pareto.mme.RealData=quantile(mme.fp.RealData, probs = c(0.5, 0.75, 0.95))
#trgamma
mle.ft.RealData <- fitdist(Sample.RealData, "trgamma", method="mle", optim.method="SANN")
Quantile.trgamma.mle.RealData=quantile(mle.ft.RealData, probs = c(0.5, 0.75, 0.95))
memp  <-  function(x, order) mean(x^order)
mme.ft.RealData <- fitdist(Sample.RealData, "trgamma", method="mme", order=c(1, 2, 3), memp=memp)
Quantile.trgamma.mme.RealData=quantile(mme.ft.RealData, probs = c(0.5, 0.75, 0.95))
```

```{r, echo=FALSE, include=FALSE, comment=NA}
Table.AIC=matrix(0,8,1,dimnames=list(c("lnorm","exp","gamma","Weibull","llogis","invWeibull","Pareto","trgamma"),c("AIC (MLE,Sample.RealData)")))
Table.AIC[1,]=mle.fln.RealData$aic
Table.AIC[2,]=mle.fe.RealData$aic
Table.AIC[3,]=mle.fg.RealData$aic
Table.AIC[4,]=mle.fw.RealData$aic
Table.AIC[5,]=mle.fl.RealData$aic
Table.AIC[6,]=mle.fi.RealData$aic
Table.AIC[7,]=mle.fp.RealData$aic
Table.AIC[8,]=mle.ft.RealData$aic
Table.AIC.min=matrix(0,1,1,dimnames=list(c("Min  (AIC)"),c("AIC(MLE,Best dist)")))
Table.AIC.min[1,]=c(rownames(Table.AIC)[which(Table.AIC[,1] == min(Table.AIC[,1]))])
```

```{r AICRDA, echo=T, comment = NA}
kable(Table.AIC, digits = 3, format.args = list(big.mark = ","), caption=("AIC summary for Building Insurance Claim Size recorded in 1990"))
```

```{r AICRDAmin, echo=T, comment = NA}
kable(Table.AIC.min, digits = 3, format.args = list(big.mark = ","), caption=("Best (or DISToo) distribution chosen through AIC for Building Insurance Claim Size recorded in 1990"))
```

```{r, echo=FALSE}
#Quantiles of real data
Quantile.best.mle.RealData.check=quantile(Sample.RealData.raw, c(0.5,0.75,0.95))
#Quantiles of best fitted model: llogis
Quantile.best.mle.RealData.fitted=Quantile.llogis.mle.RealData$quantiles*100000
#Quantile.best.mle.RealData=qllogis(c(0.5,0.75,0.95), shape=mle.fl.RealData$estimate[1], scale=mle.fl.RealData$estimate[2])*100000
TableB.1=matrix(0,2,3,dimnames=list(c("Quantiles of RealData","Quantiles of BestFitted (llogis)"),c("50%","75%","95%")))
TableB.1[1,]=Quantile.best.mle.RealData.check
TableB.1[2,]=c(Quantile.best.mle.RealData.fitted[[1]],Quantile.best.mle.RealData.fitted[[2]],Quantile.best.mle.RealData.fitted[[3]])
```
Thus, *llogis* is the **best AIC-based parametric** choice to fit the *Building* claims occurred in 1990.  The point estimates are provided in Table \@ref(tab:TableB1RDA) and gives the non-parametric and `best' AIC-based parametric quantile point estimates.  

```{r TableB1RDA, echo=T, comment = NA}
colnames(TableB.1) <- c("q(50%)","q(75%)","q(95%)")
rownames(TableB.1) <- c("Non-parametric", "AIC Parametric - llogis")
kable(TableB.1, format.args = list(big.mark = ","), caption=("Quantile point estimates for Building Insurance Claim Size recorded in 1990"))
```
It is interesting to note that the non-parametric high quantile point estimate -- at level $95\%$ -- is larger than its parametric estimate. We now use bootstrapping to understand the variability of our point estimates.


**Bootstrap 100 times** is our first setting, i.e. we generate *100* samples with replacement from the original sample of size *167*: 
$$
\tilde{x}_1,\ldots,\tilde{x}_{100}\quad \text{with}\quad dim(\tilde{x}_i)=167 \quad\text{for all}\;1\le i \le 100.
$$
We then fit the parameter(s) $\hat{\Theta}_{MLE,i}$ of **Distoo**, i.e. *llogis* in this case, based on the bootstrap sample $\tilde{x}_{i}$ for all $1\le i \le 100$. Further, we compute
$$
\hat{q}^{MLE,i}_{xx} \quad\text{for all}\quad 1\le i \le 100\quad\text{and}\quad xx\in \{50\%, 75\%, 95\%\},\\
$$
where $\hat{q}^{MLE,i}_{xx}$ is the $xx\%$ quantile of **Distoo** by assuming $\Theta=\hat{\Theta}_{MLE,i}$. Furthermore, we report 

\begin{eqnarray*} 
&&(i)\;\text{Bootstrap Estimate} \quad\hat{q}^{MLE,boot}_{xx}=\frac{1}{100}\sum^{100}_{i=1} \hat{q}^{MLE,i}_{xx},\\
&&(ii)\;\text{Bootstrap Variance} \quad\widehat{Var}\left(\hat{q}^{MLE,boot}_{xx}\right)=\frac{1}{100}\sum^{100}_{i=1}\left(\hat{q}^{MLE,i}_{xx}-\hat{q}^{MLE,boot}_{xx}\right)^2,\\
&&(iii)\;\text{MSE Estimate} \quad\widehat{MSE}\left(\hat{q}^{MLE,boot}_{xx}\right)=\frac{1}{100}\sum^{100}_{i=1}\left(\hat{q}^{MLE,i}_{XX}-\hat{q}^{MLE}_{XX}\right)^2,
\end{eqnarray*}
for all $xx\in \{50\%, 75\%, 95\%\}$.

Here are the bootstrap samples for which the sampling seed is not fixed, as it should be for a randomised experiment. 

```{r, echo=FALSE, comment=NA}
MLE.llogis.real.boot=matrix(0,100,2,dimnames=list(c(1:100),c("shape","scale")))
Quantile.llogis.real.boot=matrix(0,100,3,dimnames=list(c(1:100),c("50%","75%","95%")))
for (b in 1:100) {
Sample.boot.real=sample(Sample.RealData, replace = TRUE)
MLE.llogis.real=mledist(Sample.boot.real, "llogis")
MLE.llogis.real.boot[b,]=MLE.llogis.real$estimate

mle.fl.RealData.boot <- fitdist(Sample.boot.real, "llogis", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.llogis.mle.real.boot=quantile(mle.fl.RealData.boot, probs = c(0.5, 0.75, 0.95))
Quantile.llogis.real.boot[b,]=c(Quantile.llogis.mle.real.boot$quantiles[[1]],Quantile.llogis.mle.real.boot$quantiles[[2]],Quantile.llogis.mle.real.boot$quantiles[[3]])*100000
}
```

```{r, echo=FALSE}
#(i) Bootstrap estimation
Boot.quantile.MLE.mean=colMeans(Quantile.llogis.real.boot)
#(ii) Variance of the bootstrap estimation
Boot.quantile.MLE.variance=c(var(Quantile.llogis.real.boot[,1]),var(Quantile.llogis.real.boot[,2]),var(Quantile.llogis.real.boot[,3]))
#(iii) MSE estimate
Boot.quantile.MLE.MSE=(1/100)*c(sum((Quantile.llogis.real.boot[,1]-Quantile.best.mle.RealData.fitted[[1]])^2),sum((Quantile.llogis.real.boot[,2]-Quantile.best.mle.RealData.fitted[[2]])^2),sum((Quantile.llogis.real.boot[,3]-Quantile.best.mle.RealData.fitted[[3]])^2))
TableB.2a=matrix(0,5,3,dimnames=list(c("(i) Bootstrap estimator (n=100)","(ii) Var of boot estimator","(iii) MSE estimate","(iv) above (ii)/(i)","(v) above (iii)/(i)"  ),c("50%","75%","95%")))
TableB.2a[1,]=Boot.quantile.MLE.mean
TableB.2a[2,]=Boot.quantile.MLE.variance
TableB.2a[3,]=Boot.quantile.MLE.MSE
TableB.2a[4,]=Boot.quantile.MLE.variance/Boot.quantile.MLE.mean
TableB.2a[5,]=Boot.quantile.MLE.MSE/Boot.quantile.MLE.mean
```
```{r, echo=FALSE}
#(Case 2) \textbf{Bootstrap 150 times}
MLE.llogis.real.boot.150=matrix(0,150,2,dimnames=list(c(1:150),c("shape","scale")))
Quantile.llogis.real.boot.150=matrix(0,150,3,dimnames=list(c(1:150),c("50%","75%","95%")))
for (b in 1:150) {
Sample.boot.real.150=sample(Sample.RealData, replace = TRUE)
MLE.llogis.real.150=mledist(Sample.boot.real.150, "llogis")
MLE.llogis.real.boot.150[b,]=MLE.llogis.real.150$estimate

mle.fl.RealData.boot.150 <- fitdist(Sample.boot.real.150, "llogis", method="mle", lower = c(0, 0), start = list(shape = 1, scale = 1))
Quantile.llogis.mle.real.boot.150=quantile(mle.fl.RealData.boot.150, probs = c(0.5, 0.75, 0.95))
Quantile.llogis.real.boot.150[b,]=c(Quantile.llogis.mle.real.boot.150$quantiles[[1]],Quantile.llogis.mle.real.boot.150$quantiles[[2]],Quantile.llogis.mle.real.boot.150$quantiles[[3]])*100000
}
```
```{r, echo=FALSE}
#(i) Bootstrap estimation
Boot.quantile.MLE.mean.150=colMeans(Quantile.llogis.real.boot.150)
#(ii) Variance of the bootstrap estimation
Boot.quantile.MLE.variance.150=c(var(Quantile.llogis.real.boot.150[,1]),var(Quantile.llogis.real.boot.150[,2]),var(Quantile.llogis.real.boot.150[,3]))
#(iii) MSE estimate
Boot.quantile.MLE.MSE.150=(1/150)*c(sum((Quantile.llogis.real.boot.150[,1]-Quantile.best.mle.RealData.fitted[[1]])^2),sum((Quantile.llogis.real.boot.150[,2]-Quantile.best.mle.RealData.fitted[[2]])^2),sum((Quantile.llogis.real.boot.150[,3]-Quantile.best.mle.RealData.fitted[[3]])^2))
TableB.2b=matrix(0,5,3,dimnames=list(c("(i) Bootstrap estimator (n=150)","(ii) Var of boot estimator","(iii) MSE estimate","(iv) above (ii)/(i)","(v) above (iii)/(i)"  ),c("50%","75%","95%")))
TableB.2b[1,]=Boot.quantile.MLE.mean.150
TableB.2b[2,]=Boot.quantile.MLE.variance.150
TableB.2b[3,]=Boot.quantile.MLE.MSE.150
TableB.2b[4,]=Boot.quantile.MLE.variance/Boot.quantile.MLE.mean.150
TableB.2b[5,]=Boot.quantile.MLE.MSE/Boot.quantile.MLE.mean.150
```

We now **bootstrap 150 times** our claim data and perform the same computations.  All results are reported in Table \@ref(tab:TableB2aRDA) and \@ref(tab:TableB2bRDA).

```{r TableB2aRDA, echo=T, comment = NA}
colnames(TableB.2a) <- c("q(50%)","q(75%)","q(95%)")
rownames(TableB.2a) <- c("Bootstrap Estimate", "Bootstrap Variance" , "MSE Estimate", "Bootstrap Variance / Bootstrap Estimate", "MSE Estimate / Bootstrap Estimate")
kable(TableB.2a, format.args = list(big.mark = ","), caption=("Summary of bootstrap estimation -- 100 bootstrap samples"))
```


```{r TableB2bRDA, echo=T, comment = NA}
colnames(TableB.2b) <- c("q(50%)","q(75%)","q(95%)")
rownames(TableB.2b) <- c("Bootstrap Estimate", "Bootstrap Variance" , "MSE Estimate", "Bootstrap Variance / Bootstrap Estimate", "MSE Estimate / Bootstrap Estimate")
kable(TableB.2b, format.args = list(big.mark = ","), caption=("Summary of bootstrap estimation -- 150 bootstrap samples"))
```





